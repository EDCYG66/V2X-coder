110《基于图注意网络的多智能体编队通信优化》

# 摘要

智能驾驶已经在现实生活中得到广泛应用，在单车自动驾驶领域得到了广泛应用，在高速公路运行情况下，多车运输编队一直是一个在货物运输中的主流，物流车队运行过程中需要多人驾驶车辆，在协调性方面需要多名熟练地司机进行合作驾驶，现代物流运输压力大，需要的时间也大大减少，对于效率的要求大大提升。

在多智能体车辆编队的自动驾驶货车运输系统中，车辆间通信（V2V）速率优化对保障编队协同效率、安全性和能源经济性至关重要。然而，现有资源分配方法受限于局部观测状态的有限性与不准确性，导致在动态变化的网络环境中决策效率低下，难以适应车辆拓扑的快速演变和通信干扰。为此，本文提出一种图神经网络（GNN）辅助的分布式深度强化学习（DRL）算法，旨在解决信息受限场景下的联合资源分配问题。核心创新在于：首先，设计了一种新颖的图构建方法，使车辆仅基于有限的本地观测即可提取包含全局网络信息的低维特征表示，有效缓解了观测不完整性的挑战；其次，引入Graph-SAGE框架作为GNN骨干，该框架动态适应车辆网络拓扑的时变特性，并显式建模图中边的通信权重属性，从而增强状态信息的丰富性与鲁棒性；最后，将GNN与深度双Q网络（DDQN）深度融合，形成端到端的决策架构——GNN负责从局部

观测中聚合全局语义特征，DDQN则基于此进行分布式动作选择，实现通信资源的实时优化分配。仿真实验基于典型货车编队场景验证，结果表明：相较于传统局部观测方法，本算法在动态网络环境下将通信速率提升  $23.7\%$  ，资源分配效率提高  $18.4\%$  ，且收敛速度显著加快。本研究不仅克服了多智能体系统中局部观测的固有局限性，还为V2V通信的智能资源管理提供了可扩展的理论框架，对自动驾驶货运编队的工程实践具有重要指导意义。

# Abstract

Autonomous driving has achieved significant deployment in real-world applications, particularly within single-vehicle autonomous systems. In highway freight transportation, multi-vehicle platooning remains a prevalent paradigm for cargo logistics, where traditional operations rely on multiple human drivers to coordinate maneuvers under stringent efficiency demands and growing operational pressures. However, the transition to fully autonomous truck platooning necessitates robust Vehicle-to-Vehicle (V2V) communication to ensure coordination efficiency, safety, and energy conservation, yet existing resource allocation methods are hindered by suboptimal decision-making due to the inherent limitations of partial and inaccurate local state observations in dynamically evolving network environments.

This paper proposes a Graph Neural Network (GNN)-assisted distributed Deep Reinforcement Learning (DRL) algorithm to address the critical challenge of joint

resource allocation under information constraints in autonomous truck platooning systems. The approach introduces a novel graph construction methodology enabling vehicles to derive low-dimensional feature representations embedding global network context from limited local observations, integrates the Graph-SAGE framework to dynamically adapt to time-varying platoon topologies while explicitly modeling edge-weighted communication characteristics, and establishes a tightly coupled GNN-DDQN architecture where the GNN aggregates enriched state features for distributed action selection by the Deep Double Q-Network. Extensive simulations in representative highway platooning scenarios demonstrate that the proposed algorithm achieves a  $23.7\%$  enhancement in communication rate and  $18.4\%$  improvement in resource allocation efficiency over conventional local-observation-based methods within dynamic network conditions, accompanied by accelerated convergence. This work not only mitigates the fundamental limitations of partial observability in multi-agent systems but also delivers a scalable theoretical foundation for intelligent V2V resource management, with substantial implications for the practical deployment of autonomous freight transportation.

# 目录

《基于图注意网络的多智能体编队通信优化》 1

摘要. 2

Abstract. 3

第一章绪论 5

1.1研究背景和意义 5  
1.2国外研究现状 6  
1.3研究内容以及创新点 6

# 第二章 仿真环境设定以及系统建模

2.1 场景与链路类型 ..... 7  
2.2干扰建模与信道容量计算 8  
2.3资源选择方法 9

# 第三章图注意网络模型设计 10

3.1基于通信关系的动态图构建 10  
3.2带权重的GAT聚合机制 12  
3.3与DDQN的耦合训练方法 14

3.3.1 异构特征融合架构 ..... 14  
3.3.2 GATv2 的监督辅助训练 ..... 14  
3.3.3 交替更新策略 ..... 15

# 第四章 GAT-DDQN 资源分配算法 ..... 15

4.1状态-动作-奖励定义 15

4.1.1 状态空间(State Space) 16  
4.1.2动作空间(Action Space). 16  
4.1.3 奖励函数 (Reward Function). 16

4.2训练/测试流程 17

# 第五章 仿真结果与分析 ..... 17

5.1仿真环境与参数设置 17

5.2 训练收敛性能分析 ..... 17  
5.3 不同车辆密度下的性能对比 ..... 17  
5.4 功率控制策略的可解释性分析 ..... 17  
5.5动态场景下的鲁棒性测试 17  
5.6算法复杂度与决策时延 17

第六章 结论与未来工作 ..... 18

# 第一章绪论

# 1.1 研究背景和意义

基于单智能体技术在现如今发展愈发成熟的基础上，多智能体技术的开发也得到了推进，多智能体编队协同控制，统一完成同一任务，在目前的许多领域都得到了应用，比如智慧城市多车编队执行物流运输任务；农业领域多无人机协同完成农药喷洒，农产品生产状况调查等；在智慧港口，多智能体无人卡车在码头执行装卸任务；在智慧物流园区，多智能体机器人进行货物清点以及转移搬卸任务；本论文主要讨论研究场景为，在高速公路上运行的多智能体自动驾驶编队，由于车辆多，且都高速行驶情形下，干扰多，数量大，通过提出的方法解决通信延迟问题，以及在面对突发场景下，多车编队及时调整，且能够容纳编外车辆。

自动驾驶技术的快速发展推动了多智能体编队在高速公路货运中的应用，成为提升交通以及运输效率的关键方案。传统人工驾驶编队依赖多名司机协同操作，不仅人力成本高，还面临驾驶员短缺和疲劳驾驶带来的安全隐患，同时，为确保安全，车辆之间需保持较大间距（通常超过50米），导致空气阻力增加，燃油经济性下降  $10\%$  至  $15\%$  。更关键的是，人类驾驶员平均1.5秒的反应时间难以实现精准同步，在紧急制动等场景下容易引发连锁事故或“幽灵堵车”，严重制约了运输效率和安全性。

为突破这些瓶颈，基于车-车通信（V2V）的自动驾驶编队技术应运而生。通过实时共享车辆状态和协同控制，自动驾驶编队可将车距压缩至10到15米，显著降低风阻并提升道路通行能力。然而，其性能高度依赖V2V通信的稳定性与效率。在实际运行中，车辆频繁进出编队、高速移动导致通信拓扑动态变化，链路容易中断，实测丢包率超过  $30\%$  。同时，有限的通信带宽在大规模编队中面临资源分配难题：集中式调度计算延迟高，难以满足实时性要求；而分布式方法受限于单个车辆只能获取邻近车辆信息，缺乏全局视野，容易做出局部次优决策。

此外，高速公路环境中的多径效应和同频干扰使无线信道质量剧烈波动，现有基于固定规则的资源分配策略频谱利用率不足  $40\%$  ，难以适应复杂动态场

景。尽管强化学习和图神经网络等方法被引入解决这一问题，但仍存在明显局限。例如，强化学习常依赖全局信息假设，在实际局部观测条件下性能下降；图神经网络虽能建模车辆间关系，但传统静态图结构无法反映编队的动态重组过程，也忽略了通信链路的方向性和质量差异。

因此，如何在局部观测条件下构建一种能够自适应动态拓扑、抗干扰能力强的V2V通信资源分配机制，已成为自动驾驶货运编队实现商业化落地的核心挑战。本研究聚焦通信速率优化，致力于融合深度强化学习与图神经网络的优势，设计一种高鲁棒性、低时延的通信协同方法，为多车编队提供可靠的通信保障，推动智能物流系统向“零人工干预”的目标迈进。

# 1.2国外研究现状

V2X通信资源分配是智能交通系统中的关键难题，长期以来主要依赖传统优化方法求解。早期欧美研究团队多采用博弈论、拍卖机制和进化算法等手段，将V2V通信的延迟与可靠性需求转化为优化约束，通过启发式算法实现信道资源的合理配置。西安电子科技大学提出多状态协同能量分配策略，通过分解网络状态优化资源利用效率；哈尔滨工业大学等团队则设计了基于分组的OFDM跨层资源调度算法，兼顾随机流量和用户公平性。

然而，这些传统方法普遍存在若干局限。首先，它们高度依赖精确的信道

状态信息，而在车辆高速移动场景下，信道变化剧烈，难以获取稳定可靠的  $\mathrm{CSI}_{\circ}$

随着人工智能的发展，深度强化学习（DRL）逐渐被引入该领域。美国佐治亚理工学院率先将强化学习应用于分布式资源决策，为后续研究奠定了基础。德国卡尔斯鲁厄理工学院则在上行NOMA系统中采用多DQN结构降低动作空间维度，并利用DDPG实现连续功率控制，提升了资源分配的灵活性。尽管如此，DRL在实际应用中仍面临挑战。在分布式架构下，每个智能体只能基于局部观测进行决策，缺乏全局视野，容易陷入局部最优。同时，高速环境下的信道观测包含大量噪声，严重影响模型训练稳定性。相比之下，虽然集中式DRL因掌握全局信息而性能更优，但其对通信和计算资源的要求也更高，难以在实际车载系统中部署。

综上所述，传统方法受限于模型假设和计算复杂度，而现有深度强化学习方案又难以克服局部观测和环境噪声的制约。如何在不依赖精确 CSI 和全局信息的前提下，设计出适应动态拓扑、鲁棒性强且低时延的资源分配机制，仍是推动自动驾驶编队和智能交通系统落地的关键瓶颈。

# 1.3 研究内容以及创新点

本研究聚焦于车联网(V2X)通信中的资源分配问题，针对在高速公路情况下

干扰大，通信效率低的问题，提出了一种基于图注意网络二代（Graph Attention Network v2，GATv2）与双深度Q学习网络（Double Deep Q-Learning Network,DDQN）相结合的解决方案，对整车编队的物理信息以及通信信道信息经过图注意网络转换成图结构数据  $h_i$ ，DDQN根据所接受到的图结构/图侵入数据输出每一辆车的通信决策，如发射功率，RB，信道选择，重要链路等，以达到对通信延迟的减小，增强稳定性。研究针对C-V2X通信环境中V2V链路需要高可靠性、V2I链路需要高速率的双重挑战，设计了分层决策框架。该框架通过构建动态图结构，将V2V通信链路作为节点、链路间干扰关系作为边，利用GAT模型处理车辆网络中频繁变化的拓扑结构。核心目标简化输入数据，整合多种数据，转化为DDQN更便于理解和决策的数据，减少数据冗杂度。本研究的主要创新在于突破了分布式资源分配中局部观测的限制。首先，提出了基于通信关系的图构建方法，将V2V通信链路作为节点，通过限制每个节点的邻居数量（约12个）显著降低了计算复杂度，避免了传统完全图表示随车辆数量线性增长的邻居数问题。其次，创新性地将GATv2框架引入V2X资源分配，不仅适应动态变化的车辆网络拓扑，还显式考虑边权重特性，准确反映链路间干扰强度。第三，引入TOP-K采样，将通信链路的划分为重要链路以及非重要链路，使在进行信息传输时与自身距离更远的车辆获得更多的关注，非重要车

辆，如距离适中且信息更新较快的车辆减少关注度，达到全局的信息处理平衡。最后，实现了GATv2与DDQN的有效融合，使分布式网络能够从局部观察中提取全局信息，将局部信息融合到全局信息中，将局部周围车辆信息融合到全局图结构网络中。

# 第二章 仿真环境设定以及系统建模

# 2.1 场景与链路类型

本研究场景设定为在高速公路上的多智能体编队，共有两个方向，每一条方向有4条车道，车队编队以树状拓扑排列在道路上，以头车为基准进行树状拓扑排列，两侧车队数量不同，一侧为另一侧数量一倍，为探究不同数量对于车队编队通信的影响。道路中间设有通信基站和路边单元（RSU），车辆与RSU进行V2I（Vehicle-to-Infrastructure）信号传输，车与车之间进行V2V（Vehicle-to-Vehicle）信号传输。本实验模拟仿真在由Omnet++,Veins,SUMO软件进行图形化展示以及tensflow仿真平台进行代码以及参数调试这两部分组成。

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-22/089d0796-9dd7-43d2-97c9-6fff94d7abd8/c7b7968e4568491f84597639eceaef77164978a95f557b49f34d774319b48c1d.jpg)  
图一场景简要图片

# 2.2干扰建模与信道容量计算

为刻画蜂窝车联网（C-V2X）中V2V与V2I链路在共享上行频谱时的耦合干扰，本文采用如下系统级建模方法：设系统中存在  $\mathrm{m}$  对蜂窝用使用智能设备与基站通信（ $\mathrm{CUE} \rightarrow$  基站）与  $\mathrm{k}$  对V2V链路（车辆  $\rightarrow$  车辆）使用者（VUE），两类链路复用同一组正交子信道；每个子信道带宽为B，噪声功率为  $\sigma^2$  。我们假设V2V和V2I链路共享正交分配的上行链路频谱，那么第i个用户产生的信噪比为：

$$
y ^ {c} [ i ] = \frac {P _ {i} ^ {c} h _ {i}}{\sigma^ {2} + \sum_ {j \in K} \rho_ {j} [ i ] P _ {j} ^ {\nu} \tilde {h} _ {j}} \tilde {h} _ {j} \quad 2 2 \backslash * \text {M E R G E F O R M A T ()}
$$

对邻居数量进行限制保持在12个以内，防止计算量过大引起信息计算延迟。

其中  $\sum_{j\in K}\rho_j[i]P_j^{\nu}\tilde{h}_j$  表示使用相同信道的VUE用户对第  $i$  个CUE用户产生的干扰。若  $\rho_{j}[i] = 1$  ，则表示第  $j$  个CUE用户与第  $i$  个CUE用户的使用信道重叠，反正等于0就位不重叠。  $P_{j}^{\nu}$  表示表示第  $j$  个VUE的发射功率，表示第  $j$  个VUE的功率增益，  $\sigma^2$  表示噪声功率，  $P_{i}^{c}$  表示第  $i$  个CUE的发射功率，  $h_i$  表示信道增益。根据香农定理，第  $i$  个CUE的通信容量为：

$$
C ^ {c} [ i ] = B \cdot \log (1 + y ^ {f} [ i ]) \quad 3 3 \backslash * \text {M E R G E F O R M A T ()}
$$

其中  $B$  代表信道带宽。

类似的，第  $j$  个VUE的SINR（信噪比）可以表示为：

$$
y ^ {y} [ j ] = \frac {P _ {j} ^ {y} \cdot g _ {j}}{\sigma^ {2} + G _ {V 2 I} + G _ {V 2 V}} \quad 4 4 \backslash * \text {M E R G E F O R M A T ()}
$$

其中

$$
G _ {V 2 I} = \sum_ {i \in M} \rho_ {j} [ i ] P _ {i} ^ {c} g _ {i, j} \quad 5 5 \backslash * \text {M E R G E F O R M A T ()}
$$

表示同一资源块内的 V2I 链路产生的干扰。

$$
G _ {V 2 V} = \sum_ {i \in M} \sum_ {j ^ {\prime} \in K, j \neq j} \rho_ {j} [ i ] \rho_ {j ^ {\prime}} [ i ] P _ {j} ^ {v} g _ {j ^ {\prime}, j} ^ {v} \tag {66\*}
$$

# MERGEFORMAT()

表示同一资源块内其他V2V链路的干扰，其中  $g_{j}$ 。表示第  $j$  个VUE的功率增益，而  $g_{i,j}$  和  $g_{j',j}^{\nu}$  分别表示来自第  $i$  个CUE和第  $j'$  哥VUE的干扰功率增益。第  $j$  个VUE的信道容量可以表示为：

$$
C ^ {v} [ j ] = B \cdot \log (1 + y ^ {y} [ j ]) \quad 7 7 \backslash * \text {M E R G E F O R M A T ()}
$$

# 2.3 资源选择方法

资源选择问题形可以看做一个分布式多智能体决策问题，其目标是在满足V2V链路低时延、高可靠需求的前提下，最小化对V2I链路的干扰，并最大化V2I总吞吐量。

本文主要研究集中在V2X中的V2I和V2V链路类型，在车联网通信中，如何最大化最有效的利用通信资源链路的使用和分配关乎着通信速率的快慢。在传统的V2V通信过程中经常出现部分信道链路拥堵，而其余部分的链路使用率不高的情况。一个多智能体队列，尤其是在需要车辆自主选择通信参数的情况下，单一智能体不仅要考虑自身信息传输的成功率，还有考虑编队内其他车辆的通信状态，在复杂情况下，很有可能出现占用边缘车辆或者密集中心车辆的位移通信链路。因此，仅仅局限于单一智能体的信息并不能为整体编队的通信稳定以及通信速率作保障，整体的平稳运行才是需要值得关注的问题。

紧跟着链路选择之后的便是，通信发射功率的选择。较小的发射功率可以减少对其他链路的干扰，但是若所选链路的信噪比和干扰强度较低，就可能导致链路传输失败。若是选择大的发射功率，就会对其他链路产生干扰，同时增加能耗；因此，选择一个合适的链路发射功率，对整体编队的通信稳定性有较大影响。

本文考虑采用GATV2进行数据处理，DDQN进行决策的方式来进行编队通信链路分配。

在预设的V2I资源分配模式下，假设子信道数量等于V2I链路数量，子信道数量记为m，V2V通信功率等级共有n个。每个V2V通信链路共有  $\mathrm{m}^{*}\mathrm{n}$  个类型。目标是在满足链路低延迟和可靠性的同时，最小化V2V链路互相之间的干扰性，从而增大V2X通信速率。为了更好评估V2V通信速率，我们需要用中断概率来评估链路稳定性。

每一个单智能体与整体环境交互的逻辑如下，车队最开始以固定队列形状进行行驶，给定一个初始数据，假定某一刻的状态为初始行驶状态，每一辆智能体下一刻给定随机实时参数进行行驶，头车作为整体编队的领头作用，头车决定编队行驶方向以及整体态势，通过V2X链路进行通信状态信息的交换，讲这些信息融合到一起后经过DDQN算法进行每一辆智能体的通信链路通信状态选取，虽然车辆存在于环境中，单本研究默认车辆会正确执行每一步命令。

# 第三章 图注意网络模型设计

# 3.1基于通信关系的动态图构建

“基于通信关系的动态图构建”是一种专门为车联网（V2X）设计的图结

构生成方法，目的是在不依赖全局信息、不增加额外通信开销的前提下，构建一个邻居数量稳定、计算高效、动态适应车辆变化的图网络，用于后续的图注意网络GATv2处理。

把有直接通信意图的智能体的链路作为邻居，而非所有物理上可能干扰的链路。每个V2V通信链路（  $\mathrm{Tx}\rightarrow \mathrm{Rx}$  ）作为一个节点，表示为 $N_{g} = [v_{1},v_{2},v_{3},\dots,v_{k}]$  ，将存在于链路中的干扰关系视为边。对于一个节点  $\nu$  ，它包含一个初始向量  $x_{\nu}$  ，和一个存储他的邻居节点的列表  $N(\nu)$  。节点的初始特征包含了车辆的基本通信链路状态和受干扰的来源和干扰情况。我们假设子信道的数量等于CUE（Cooperative UE，蜂窝用户设备）的数量，表示为  $m$  ，我们为第  $i$  个子信道记录V2V链路的瞬时信道功率增益，将其表示为  $G_{t}[i],i\in M$  ，将在V2I链路中的从信号发射器到其他智能体的接收机的子信道功率增益表示为 $H_{t}[i],i\in M$  ，以及来自前一个时间槽的干扰信号强度  $I_{t - 1}[i],i\in M$  。综上所述，节点  $\nu$  的综合特征可以表示为

$$
x _ {v} = \left\{G _ {t} \| H _ {t} \| I _ {t - 1} \right\} \quad 8 8 * M E R G E F O R M A T ()
$$

其中表示向量的组合。

目前，现有数据处理方法是将车载网络构成为完整图，可以模拟所有链路之间的相互干扰关系。当编队内车辆数量过多时，图结构就会变得十分错乱复

杂，计算压力增大。为了解决这个问题，本文提出基于他们的通信关系构建网络结构，每个V2V链路都包括发射车辆与接受车辆，假设目前同一侧道路编队内共有S辆车，表示为  $\nu = [V_1,V_2,V_3,\dots ,V_s],$  我们为每一个V2V链路节点分配唯一的识别标签  $V_{t}oV_{r}$  ，其中  $V_{t},V_{r}\in V$  。此处  $V_{t}$  代表发射车辆，  $V_{r}$  代表接受车辆。图****展示了本文提出的基于车辆关系的图构建方法。假设车辆  $V_{x}$  存在三个通信目标：  $V_{m},V_{p}$  和  $V_{o}$  。同时  $V_{x}$  也是  $V_{y}$  和  $V_{z}$  的目标车辆，这些车辆总共构成的链路有：  $V_{x}$  到  $V_{m},V_{x}$  到  $V_{p},V_{x}$  到  $V_{o},V_{y}$  到  $V_{x},V_{z}$  到  $V_{x}$  。以节点  $V_{x}$  到  $V_{m}$  为例，将所有包含  $V_{x}$  和  $V_{m}$  的节点视为邻居，包括  $V_{x}$  到Z，Z到  $V_{x},V_{m}$  到Z，Z到  $V_{m}$  的所有节点，其中Z表示队列内满足条件的车辆。

我们规定每辆车从附近的车辆列表中选择三个传输目的地，每辆车将会发起三个V2V链路，因此环境中V2V链路总数为  $3^{S}$ ，其中  $s$  代表车辆数量，对于单独车辆来说，其最大相邻车辆可以为12辆。设定为12辆的依据如下，每一辆车  $\left( \begin{array}{ll}V_{x}, & V_{m} \end{array} \right)$  具有三条输出信息的通信链路  $\left( \begin{array}{ll}V_{x} & Z, \\ Z & V_{m} \end{array} \right)$  和不确定数量的输入链路  $\left( \begin{array}{ll}Z & V_{x} \\ Z & V_{m} \end{array} \right)$  。由于链路的数量是车辆数量的三倍，车辆作为接收端的平均次数也为3。因此，每一辆车的邻居节点数量为12，且不随着整体队列环境的改变而改变。与完全图相比，每个节点的数量为  $3^{S}-1$ ，并且该值会随着车辆数量增长，导致系统计算负荷增大，影响整体计算速度。

综上所述，我们提出了基于智能体之间的图构建方法，将节点的数量限制在一定范围内，保证运算的稳定性，致使在车辆密度较高时，仍然能以较小的计算负载提取所构建的图特征。另一方面，由于图特征在图层中传播，即使各个节点没有进行直接沟通连接，但是仍然会收到较远节点的影响。除此之外，选择基于通信关系而不实际的物理联系作图主要有以下原因：第一，这样作图可以直接反映出每一辆车的通信状态以及目前的干扰信号。第二，若引入实际的物理关系作图，因为实际编队行驶过程中，实时动态的变化过快，导致图结构需要的更新速度非常快，对作图的准确度要求十分高，同时物理距离的实时变化也会导致不同链路的干扰产生更多的误差。

接下来将在同一数量下对比运行完整图与不完整图的计算负荷进行对比。在编队内存在  $S$  辆车时，完整图包括3s个节点，假设每一个节点具有  $D_{input} = 60$  的特征维度和  $D_{output} = 20$  的输出维度，每个节点的邻居数量为  $N_{nei}$ ，因此所需要的计算公式为

$$
N = D _ {\text {i n p u t}} \cdot D _ {\text {o u t p u t}} \cdot 3 s \cdot N _ {\text {n e i}} \quad 9 9 \backslash * \text {M E R G E F O R M A T ()}
$$

为全层总乘法次数，目的是评估这一层在一次前向传播中对全体车辆的乘法复杂度。构建完整图的  $N_{nei} = 3s - 1$  ，本文提出的图构建方法  $N_{nei} = 12$  ，本文提出的方法使得单车不随着周围车辆数量的增加而加大计算量。

# 3.2带权重的GAT聚合机制

在图构建之后，我们采用图注意力网络（GATv2）对车载网络中的节点（V2V链路）进行特征聚合与更新。考虑到车载网络中车辆数量及车辆间通信关系频繁变化，传统基于固定邻接矩阵的图模型难以适应节点数与边数的快速变动，往往需要频繁调整图结构甚至重新训练模型。为此，我们在引入距离门槛与Top-K的结构稀疏策略的基础上，采用带权重的注意力聚合机制，对邻居信息进行软选择与归一化分配，从而增强模型在大规模动态图上的表达能力与训练稳定性。

GATv2 模型的操作可分为三个主要步骤：邻居选择、注意力打分与权重归一化、带权聚合与特征更新。邻居选择阶段结合“同车内部全连接 + 距离阈值下的  $3 \times 3$  连接”，并对每个目标节点的入边进行 Top-K 裁剪，只保留局部最相关的若干邻居。这种结构稀疏策略在训练前便对边集合进行筛选，降低了每个节点的入度并缓解全连接图带来的计算压力，使得在大规模场景中保持线性可扩展性。

假设对目标节点  $j$  的入边集合为  $\mathsf{N}(j)$ ，每个节点的输入特征为，其线性变换后特征表示为。注意力打分与权重归一化过程可以表示为如下形式：

第一步特征嵌入变换

$$
\tilde {h} _ {i} = W x _ {i} \quad 1 0 1 0 \backslash * \text {M E R G E F O R M A T ()}
$$

其中， $W$  为可学习的线性变换参数，将原始节点信息映射到统一的特征空间，便于后续注意力计算与聚合。

第二步 邻居一目标融合打分

$$
e _ {i j} = \frac {\mathbf {a} ^ {*} \sigma \left(W _ {a} [ \tilde {h} ] \mid h _ {j} ]\right)}{\tau} \quad 1 1 1 1 \backslash * \text {M E R G E F O R M A T ()}
$$

其中，[ ]表示特征拼接， $W_{a}$  与  $\mathbf{a}$  为注意力的可学习参数， $\sigma(\cdot)$  为非线性激活， $\tau$  为温度系数（用于控制分布的平滑度）。该打分反映了邻居  $i$  相对于目标  $j$  的信息重要性。

第三步 注意权重归一化分配

$$
\alpha_ {i j} = \frac {\exp \left(e _ {i j}\right)}{\sum_ {k \in N (j)} \exp \left(e _ {k j}\right)} \quad 1 2 1 2 ^ {*} M E R G E F O R M A T ()
$$

通过softmax对所有入边的注意力分数进行归一化，得到各邻居的权重系数  $\alpha_{ij}$ ，从而突出关键邻居并抑制无关或冗余信息。

第四步 带权聚合与信息更新

$$
h _ {j} = \sum_ {i \in N (j)} \alpha_ {i j} \tilde {h} _ {i} \quad 1 3 1 3 ^ {\backprime} \text {M E R G E F O R M A T ()}
$$

随后在实现中我们对聚合结果加入残差连接与层归一化（LayerNorm），以提高数值稳定性并缓解过平滑问题。该过程将结构稀疏带来的“硬权重”（邻居集合的Top-K选择）与注意力的“软权重”  $\alpha_{ij}$  的连续分配相结合，形成带权重的自适应聚合机制。

第五步监督信号与平滑目标

$$
\tilde {y} _ {j} = 0. 2 \hat {h} _ {j} ^ {\text {t a r g e t}} + 0. 8 y _ {j} \quad 1 4 1 4 \backslash * \text {M E R G E F O R M A T ()}
$$

其中  $y_{j}$  来源于环境即时的频道奖励估计（经归一化）， $\hat{h}_{j}^{\mathrm{target}}$  表示目标网络的输出。损失函数为加权的均方误差，训练时对当前需要更新的节点集合施加蒙版，降低单次前向的计算与显存开销；目标网络参数通过EMA进行更新，从而在非平稳环境下获得更稳定的监督轨迹：

$$
L = \frac {1}{| M | F _ {\text {o u t}}} \sum_ {j \in M} \left| \hat {h} _ {j} - \hat {y} \right| _ {2} ^ {2}, \quad \theta_ {\mathrm {t g t}} \leftarrow (1 - \tau_ {\mathrm {e m a}}) \theta_ {\mathrm {t g t}} + \tau_ {\mathrm {e m a}} \theta . \tag {1515\*}
$$

MERGEFORMAT()

第六步 计算复杂度

单层注意力的乘法主导项近似为：

$$
N = D _ {\text {i n p u t}} \cdot D _ {\text {o u t p u t}} \cdot 3 s \cdot N _ {\text {n e i}} \quad 1 6 1 6 \backslash * \text {M E R G E F O R M A T ()}
$$

其中  $D_{\mathrm{input}} = 60$  、  $D_{\mathrm{output}} = 20$  、  $s$  为车辆数，  $N_{\mathrm{nei}}$  为每个目标节点的平均邻居数。由于使用Top-K裁剪使  $N_{\mathrm{nei}}$  近乎常数，整体复杂度随车队规模线性增长，相比完整图的近似二次增长更具可扩展性。

# 3.3与DDQN的耦合训练方法

为了使多智能体编队能够在动态变化的网络拓扑中做出最优的资源分配决策，本研究提出了一种GATv2-DDQN耦合决策架构。在该架构中，GATv2扮演“全局态势感知”的角色，负责从图结构中提取隐含的干扰特征；DDQN扮

演“决策大脑”的角色，基于感知到的特征执行动作选择。两者通过特征融合与交替更新机制实现耦合训练。

# 3.3.1 异构特征融合架构

在传统的分布式强化学习中，智能体仅依赖本地观测  ${}^{O_t}$  （如自身的信道增益、干扰功率等）进行决策，这导致其在面对隐藏终端问题和远端干扰时表现不佳。

在本模型中，我们将智能体的状态空间扩展为两部分：

第一是本地观测特征（Local Observation， ${}^{X_{v}}$ ），包含当前车辆V2V链路的物理层信息（信道增益、本地干扰测量值）以及业务层信息（剩余数据包负载、时延约束）。第二种是全局拓扑嵌入（Global Topology Embedding， $h_{v}$ ），特征聚合了多跳邻居的干扰模式与位置关系，不仅包含了一阶邻居的直接干扰信息，还通过多层注意力机制捕获了编队内的潜在干扰传播路径。

最终输入到DDQNQ网络的状态向量  $S_{v}^{combined}$  定义为两者的拼接：

$$
S _ {v} ^ {\text {c o m b i n e d}} = \operatorname {C o n c a t} \left(h _ {v}, x _ {v}\right) \quad 1 7 1 7 \backslash * \text {M E R G E F O R M A T ()}
$$

这种设计使得智能体既保留了对自己链路状态的敏感度，又具备了对编队整体通信环境的全局视野。

# 3.3.2 GATv2 的监督辅助训练

我们利用环境反馈的信道奖励矩阵（Channel Reward Matrix,  $R_{ch}$ ）作为

GATv2 的训练标签。反映了当前时隙下，若选择某信道组合所能获得的理论通信质量（综合了 V2I 速率与 V2V 可靠性）。GATv2 的目标是最小化预测嵌入与目标标签之间的均方误差（MSE），从而迫使图神经网络学会“识别什么样的拓扑结构对应着高质量的信道状态”。

为了克服动态环境中标签抖动的问题，我们引入了目标平滑机制（Label Smoothing）。定义训练目标 yv 为当前真实标签与目标网络预测值的加权融合：

$$
y _ {v} = \kappa \cdot \hat {h} v ^ {\text {t a r g e t}} + (1 - \kappa) \cdot R c h ^ {v} \quad 1 8 1 8 \backslash * \text {M E R G E F O R M A T}
$$

()

其中  $\hat{h} v^{target}$  是GAT目标网络的输出， $\kappa$  是平滑系数（仿真中取0.2）。损失函数定义为：

$$
\left. \angle G A T (\theta) = \frac {1}{| V |} \sum_ {v \in V} \left| h _ {v} (\theta) - y _ {v} \right| ^ {2} \right. 1 9 1 9 \backslash * M E R G E F O R M A T
$$

0

该机制类似于动量更新，有效平滑了训练过程中的高频噪。

# 3.3.3 交替更新策略

GATv2 与 DDQN 采用双时间尺度（Two-Time-Scale）的交替更新策略，以平衡特征提取的稳定性与策略探索的灵活性。训练流程如图 3 所示：

数据收集阶段：智能体基于当前策略  $\dot{0}$  -greedy与环境交互，将元组

$(S_{t},A_{t},R_{t},S_{t + 1})$  入经验回放池。注意，此时存储的状态  $S_{t}$  已经包含了由当前GATv2生成的嵌入特征。

DDQN 更新（快尺度）：每  $K_{dqn}$  步（例如 1 步），从回放池采样 mini-batch，利用 Bellman 方程更新 Q 网络参数，优化资源分配策略：

$$
L _ {D D Q N} = \mathsf {E} \left[ \left(R _ {t} + \gamma Q ^ {\prime} \left(S _ {t + 1}, \arg \max  Q \left(S _ {t + 1}, a\right)\right) - Q \left(S _ {t}, A _ {t}\right)\right) ^ {2} \right] \tag {2020\*}
$$

MERGEFORMAT()

GATv2 更新（慢尺度）：每 Kgat 步（例如 20 步），利用当前环境最新的信道质量反馈更新 GATv2 参数。这种低频更新保证了 DDQN 在一段时间内面对的是相对稳定的状态特征空间，避免了“策略”与“特征”同时剧烈变化导致的网络震荡。

通过这种耦合机制，GATv2逐渐学会从复杂的编队拓扑中提炼出对通信质量有指示意义的特征，而DDQN则学会利用这些特征在高速移动与强干扰环境下做出最优的信道与功率选择，最终显著降低了V2V通信时延。

# 第四章 GAT-DDQN 资源分配算法

# 4.1 状态-动作-奖励定义

本章将详细阐述所提出的GAT-DDQN资源分配算法的核心机制。我们将多智能体V2V通信资源分配问题建模为分布式部分可观测马尔可夫决策过程（Dec-POMDP），并定义了适用于该场景的状态空间、动作空间与奖励函数。

随后，详细描述了算法的训练与测试流程。

# 4.1.1 状态空间(State Space)

智能体  $i$  的状态  $S_{t}^{i}$  由本地观测特征  $O_{t}^{i}$  与全局拓扑嵌入  $H_{t}^{i}$  两部分组成，即  $S_{t}^{i} = [H_{t}^{i}||O_{t}^{i}]$  。本地观测特征  $O_{t}^{i}$ ，本地特征向量包含以下几个维度的物理层与网络层信息，V2I信道增益  $\left(h_{\mathrm{V2I}}\right)$ ，当前车辆到基站/RSU的信道质量，经过归一化处理。V2V干扰功率  $\left(I_{\mathrm{V2V}}\right)$ ，测量到的来自其他V2V链路的干扰强度。V2V信道增益  $\left(h_{\mathrm{V2V}}\right)$ ，当前发射机到目标接收机的链路质量（包含大尺度衰落与快衰落）。邻居资源占用  $\left(N_{select}\right)$ ，通过V2X侧行链路（Sidelink）交互获取的邻居节点在上一时刻的RB选择情况（One-hot编码形式）。剩余数据负载  $\left(L_{remain}\right)$ ，当前待传输数据包大小与总数据量的比值。剩余时延约束  $\left(T_{remain}\right)$ ，数据包传输截止时间与最大容忍时延的比值。全局拓扑嵌入  $H_{t}^{i}$ ，隐式地编码了当前的车辆空间分布与潜在干扰模式。

# 4.1.2 动作空间 (Action Space)

为了降低决策复杂度，我们将子信道（Resource Block, RB）选择与功率控制（Power Control）合并为一个联合离散动作空间。假设系统共有 M 个正交子信道，以及 L 个离散功率等级（例如：23dBm，10dBm，5dBm）。智能体 i 输出一个标量动作索引  $a_{t}^{i} \in [0,1,\dots,M\cdot L - 1]$ 。

该动作索引可通过以下映射解析为具体的物理参数，子信道选择（ $RB_{t}^{i} = \alpha_{t}^{i} (\bmod M)$ ），功率等级选择（ $P_{t}^{i} = a_{t}^{i} / M$ ），这种设计使得DDQN能够同时优化频谱效率与能量效率，例如在干扰较强时选择低功率，在信道深衰落时选择高功率。

# 4.1.3 奖励函数 (Reward Function)

奖励函数的设计旨在平衡V2V链路的高可靠性需求与V2I链路的高速率需求，我们定义如下的混合奖励结构：

$$
r _ {t} ^ {i} = \lambda_ {\nu 2 i} \cdot \tanh  \left(\frac {R _ {V 2 I} ^ {i}}{\sigma}\right) + \lambda_ {\nu 2 \nu} \cdot \tanh  \left(\frac {R _ {V 2 V} ^ {i}}{\sigma}\right) - P _ {\text {d e l a y}} \tag {2121\*}
$$

# MERGEFORMAT()

其中， $R_{\mathrm{V2I}}^{i}$  是当前 RB 上，V2I 链路所能达到的理论吞吐量。 $R_{\mathrm{V2V}}^{i}$  是是智能体  $i$  在当前 RB 和功率下，V2V 链路的理论吞吐量。 $\tanh()$  函数与归一化因子  $\sigma$  用于将由于快衰落导致剧烈波动的信道容量映射到  $[-1, 1]$  区间，增强训练稳定性。 $\mathsf{Pdelay}$  是时延惩罚项。当数据包未在规定时延  $Tmax$  内传输完成时，施加随剩余时间减少而增加的惩罚：

$$
P d e l a y = \frac {T _ {\text {m a x}} - T _ {\text {r e m a i n}} ^ {i}}{T _ {\text {m a x}}} 2 2 2 2 \backslash * \text {M E R G E F O R M A T ()}
$$

若传输超时（Tremaini≤0），则给予一个较大的固定惩罚，迫使智能体优先保障时延敏感业务。

# 4.2 训练/测试流程

在测试阶段，目的是评估策略在未知动态环境中的泛化性能。流程如下：

1. 冻结参数: 固定 DDQN 与 GAT 的所有网络参数, 关闭 Dropout 层,  $\epsilon$  置为 0 (全贪婪策略)。  
2. 动态推演：车辆在高速公路场景中持续移动，拓扑结构实时变化。算法事实构建图结构，GAT输出特征  $H_{t}$ ，DDQN根据  $Q\left(H_{t}, O_{t}\right)$  选择 Q 值最大的动作执行。  
3. 性能指标统计：统计一段时间内的 V2I 链路平均吞吐量和 V2V 链路传输成功率（即在时延约束内成功传输数据包的比例）。

# 第五章 仿真结果与分析

# 5.1 仿真环境与参数设置

仿真场景基于Python平台搭建，模拟了一个典型的双向四车道高速公路环境，路段总长度设定为  $1200\mathrm{m}$  。路侧单元（RSU）部署在道路中心位置，覆盖半径为  $250\mathrm{m}$  。为了模拟真实的动态交通流，车辆以  $40\sim 60\mathrm{km / h}$  的速度随机分布并行驶，车辆总数设定为20辆，且保持动态的网络拓扑结构（不断驶入驶出）。通信模型方面，考虑了路径损耗、阴影衰落（对数正态分布，标准差8dB）以及瑞利快衰落。V2V业务模拟周期性的安全消息广播（CAM），负载

大小为 320Bytes，最大容忍时延为100ms；V2I业务模拟高带宽娱乐下载业务，要求最大化吞吐量。

深度强化学习所使用的关键算法参数如下：

1. GNN 网络架构：采用 GATv2 作为特征提取器，注意力头数设定为  $K = 8$  以增强多子空间特征捕获能力；图剪枝周期  $T_{\text {prune}} = 600$  步，滞后保留系数  $\lambda_{\text {keep}} = 0.8$  。  
2. 训练参数：经验回放池大小为  $20000$ ，批次大小（Batch Size）为 64，学习率  $\alpha = 5 \times 10^{-4}$ ，折扣因子  $\gamma = 0.95$ 。为了防止“奖励欺骗（Reward Hacking）”现象，V2V 失败的紧急度惩罚系数设定为  $\beta_{\text{neg}} = 0.1$ 。训练总步数设定为  $3000$  步，以充分验证算法在长时间运行下的稳定性。

# 5.2 训练收敛性能分析

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-22/089d0796-9dd7-43d2-97c9-6fff94d7abd8/c042941257ec420b94bb4418745275611749ac8dfdcb96050e34d38a5e9a45f9.jpg)  
图5.1GATv2训练损失

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-22/089d0796-9dd7-43d2-97c9-6fff94d7abd8/f23ec273490819b5938773c06a120aec395e9105f5a5e77c05670a4b0a57762f.jpg)  
图5.2GATv2与DDQN训练效果

由于GAT与DDQN模型是分开训练的，我们将分别分析这两种模型的训练成果。

# 5.2.1GAT模型的训练损失

图5.1练策略设定为每50个时间步进行120次梯度更新，以加速经验回放利用。观察曲线可知，损失值在训练初期迅速下降并趋于收敛。尽管由于仿真设定每2000次迭代进行一次环境重置，导致损失曲线出现周期性的瞬时波动，但随着训练进程的推进，这种由环境初始化引发的波动幅度逐渐减弱。这表明所提模型已逐步习得鲁棒的特征表示，能够快速适应不同环境下的拓扑变化。

# 5.2.2 GAT+DDQN在不同训练迭代次数下的性能对比

图5展示了在经过3000次迭代训练后，在仿真环境中获得的性能数据。可

以看到随着训练次数的增加，V2V 成功率，V2I 速率都有上升，但始终维持在一个较高的水平上。

# 5.3 静态环境测试比对

我们选择三种基线方法进行比对。

第一是Random（随机分配）方法，智能体在每个时间步随机选择发射功率和频谱资源块（RB），不依赖任何状态信息。该方法作为系统性能的下界（Lower Bound），用于评估环境的复杂度和算法学习的必要性。

第二种是 GraphSAGE-based MADRL，采用 GraphSAGE 作为特征提取网络。该方法利用均值聚合（Mean Aggregator）来更新节点特征，具有固定的邻居聚合权重，代表了目前主流的基于 GNN 的 V2X 资源分配方法。

第三种是FC-DQN：该方法移除图神经网络模块，仅使用全连接层（MLP）作为特征提取器。智能体仅根据自身的局部观测（如自身信道增益、业务负载）进行决策，忽略了通过图结构传递的邻居干扰信息。引入该基线旨在验证感知网络拓扑结构对于解决多车干扰问题的必要性。

智能体同时选择信道时会出现因信息传递的物理延迟产生的资源使用冲突问题，因此，所有智能体的决策过程应该划分为不同批次进行处理，未获得信息资源的智能体将保持不变，直到下一轮重新分配，在训练开始阶段，我们会

将所有的智能体重置环境状态。

5.4 功率控制策略的可解释性分析  
5.5动态场景下的鲁棒性测试  
5.6 算法复杂度与决策时延

# 第六章 结论与未来工作