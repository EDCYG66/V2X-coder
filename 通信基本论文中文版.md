# 基于图神经网络和深度强化学习的V2X通信资源分配

纪茂新，吴琼，Senior Member, IEEE，范平志，Senior Member, IEEE，程楠，Senior Member, IEEE，陈雯，Senior Member, IEEE，王江舟，Fellow, IEEE, Khaled B. Letaief, Fellow, IEEE

Abstract在车联网技术快速发展的背景下，蜂窝车联网通信因其在覆盖范围、时延和吞吐量方面的卓越性能而备受关注。C-V2X系统中的资源分配对于保障安全信息传输、满足车车间通信超低时延与高可靠性严格要求具有关键意义。本文提出将图神经网络与深度强化学习相结合的方法应对这一挑战：通过构建以通信链路为节点的动态图，采用GraphSAGE模型适应图结构变化，旨在确保V2V通信高成功率的同时，最小化对车路通信链路的干扰，从而既保障V2V链路信息成功传输，又维持V2I链路的高传输速率。所提方法保留了GNN的全局特征学习能力，支持分布式网络部署，使车辆能基于本地观测从图网络提取包含结构信息的低维特征，并独立做出资源分配决策。仿真结果表明，GNN的引入以适度增加计算负载为代价，有效提升了智能体的决策质量，展现出优于其他方法的性能。该研究不仅为V2V与V2I通信提供了理论高效的资源分配策略

本研究部分得到国家自然科学基金（项目号：61701197和62071296）资助；部分得到国家重点研发计划（项目号：2021YFA1000500(4)）资助；部分得到国家重大专项（项目号：2020YFB1807700）资助；部分得到上海市科委项目（项目号：22JC1404000）资助；部分获得研究资助局卓越学科领域计划（项目号：AoE/E-601/22-R）资助；部分得到111计划（项目号：B23008）资助。

纪茂鑫与吴琼就职于江南大学物联网工程学院，无锡214122（电子邮件：maox-inji@stu.jiangnan.edu.cn, qiongwu@jiangnan.edu.cn）。

范平毅就职于清华大学电子工程系，北京信息科学与技术国家研究中心，地址：北京市100084（电子邮件：fpy@tsinghua.edu.cn）。

南程就职于西安电子科技大学综合业务网理论及关键技术国家重点实验室与通信工程学院，地址：中国西安710071（电子邮件：dr.nan.cheng@ieee.org）。

陈文就职于上海交通大学电子工程系，上海200240，中国（电子邮件：wenchen@sjtu.edu.cn）。

江舟王隶属于英国坎特伯雷CT27NT肯特大学工程学院（电子邮件：j.z.wang@kent.ac.uk）。

K.B.Letaief任职于香港科技大学电子与计算机工程系（电子邮件：eekhaled@ust.hk）。

版权所有 (c) 2024 IEEE。允许个人使用本材料。然而，如需将本材料用于任何其他目的，必须通过向 pubs-permissions@ieee.org 发送请求从 IEEE 获取许可。

也为实际车联网环境中的资源管理开辟了新的技术路径。

Index Terms—车联网、资源分配、图神经网络、强化学习

# 一、引言

在智慧城市的发展中，车联网（V2X）技术在智能交通系统中发挥着至关重要的作用[1]–[3]。V2X旨在实现车辆与其周围环境（包括其他车辆、交通基础设施、行人及网络资源）的全面通信[4]–[6]。随着汽车产业向自动驾驶、智能导航、自动泊车等尖端技术过渡[7]–[11]，V2X的重要性日益凸显。然而，V2X在性能与安全性方面仍面临挑战。

鉴于这些挑战，业界已开发出多种车联网技术以应对车辆通信的不同需求[12][13]。在众多车联网技术中，蜂窝车联网(C-V2X)被认为比IEEE802.11p技术具有更高数据传输速率、更低延迟及更优可靠性。近期，第三代合作伙伴计划(3GPP)已在其Release16标准中将新空口车联网(NR-V2X)技术纳入标准化规范[14][15]。

然而，随着这些先进技术的部署，新的问题也随之出现[16]–[20]。在实际部署中，资源分配问题成为车联网技术的关键环节，需要满足车联网通信带来的巨大无线通信需求[21]–[27]。该问题通常属于NP难问题，导致在车联网环境中难以同时满足车对车链路可靠性要求与车对基础设施链路速率要求[28][29]。此外，传统资源分配方法往往依赖于精确的信道状态信息[30]，这在车辆高速移动的环境中极难实现。

为应对这些资源分配挑战，研究者们已转向采用先进算法。随着深度学习（DL）与强化学习（RL）的发展，研究人员开始利用其强大的功能

深度强化学习 (DRL) 用于解决资源分配问题的近似特性 [31]–[36]。在存在不准确的 CSI 的情况下，DRL 可以通过不断的试错积累经验，学习一种可以在分布式资源分配场景中实现更好性能的通用策略。

然而研究表明，在正常情况下，集中式资源分配方案在效能方面通常优于分布式方案[37]。这种优势源于集中式方法能够获取资源占用的全局信息及各链路的特定需求，从而有助于制定更优的分配策略。

然而，在分布式环境中，强化学习中的智能体仅能基于局部观测做出决策，这些观测往往缺乏完整的系统信息。这种局限性阻碍了车辆间的有效协作，严重影响了全局资源调度的质量。由于资源冲突与拥塞的高发性，以及通信链路间相互干扰的管理难度，该场景更趋复杂。此外，在车辆高速运动的环境中，基于局部观测获取的信道状态信息承载大量噪声[38]，难以精准刻画信道条件，进而对深度强化学习的性能产生不利影响。

为应对这些挑战，研究者开始探索图神经网络（GNNs）的潜力。图神经网络在提取全局信息和抑制噪声方面具有独特优势，这体现在链接预测[39]、节点分类[40]、图分类[41]和图生成等多种任务中的卓越表现。GNNs通过迭代聚合相邻节点特征的工作机制，使信息能够沿着边逐层传播。因此，多层GNNs既能捕获蕴含全局信息的节点特征，又能在聚合过程中有效抑制噪声。这种分层迭代的信息处理方式使GNNs能够识别图结构数据中的复杂模式与依赖关系，从而增强其在各类应用中的实用性。

事实上，文献中已有诸多研究尝试将图神经网络与深度强化学习相结合以解决复杂问题。然而，由于图神经网络独特的结构特性，现有结合方法大多采用集中式框架运作。此外，传统图神经网络难以应对图结构的动态变化——当节点数量或节点间连接关系发生改变时，往往需要对图进行多种调整，甚至需要重新训练网络。在车辆高速移动且数量频繁波动的车联网环境中，这种特性会严重损害网络的适应性与运行效率。

图采样与聚合（GraphSAGE）模型由William Hamilton、Rex Ying和Jure Leskovec于2017年提出[42]，是专门适用于动态 $\{\mathbf{v}^{*}\}$

图结构。它能够学习适用于所有节点的通用聚合函数，以处理节点数量变化的问题，同时通过每层对节点进行采样，避免因邻居节点数量过多导致的高计算成本。然而，GraphSAGE主要专注于聚合节点特征，忽略了边权重的重要性。在车联网环境中，V2V链路被表示为节点，这些链路之间的相互干扰被描绘为边，边的权重可以直观地表示干扰强度，因此成为关键参数。

此外，车载网络环境通常被表示为完整图，其中节点（V2V链路）通过干扰关系相互连接。随着环境中车辆数量的增加，图的复杂性不断升级，导致对计算资源的需求大幅增加。这种高资源消耗在低延迟是关键需求的车载网络环境中并不可行。

本文为实现更高效的分布式资源分配，并解决车辆局部观测状态有限且不精确的影响，提出了一种图神经网络辅助的分布式深度强化学习算法<sup>1</sup>。论文的主要贡献可归纳如下：

1) 为了丰富车辆自主选择资源时可用的状态信息，我们引入了Graph-SAGE框架，它能够适应动态图结构，并且我们考虑了边的特性。  
2）为构建GraphSAGE框架并实现资源分配方案，我们将图神经网络与双深度Q网络（DDQN）集成到分布式场景中。在该架构中，每个V2V链路既作为图中的节点，又充当DDQN中的智能体。此外，我们还分析了计算复杂度及系统对车辆规模增长的鲁棒性，最终通过仿真验证该方法的有效性。

本文的其余部分组织如下：第二部分回顾了相关工作。第三部分介绍了系统模型。第四部分提出了所提出的GNN模型，提供了一种新颖的图构建方法。在第五部分中，我们利用先前构建的GNN模型，基于车辆的局部观测提取包含全局信息的低维特征，并与DDQN模型协作以解决联合资源分配问题。第六部分展示了仿真结果。最后，第七部分对本文进行了总结。为了更好地理解所提出的方法，我们接下来将回顾相关方向的最新发展。

# 二、相关工作

在本节中，我们首先回顾基于传统方法的相关工作，然后介绍最新的研究成果。

<sup>1</sup>The source code has been released at: https://github.com/qiongwu86/GNN-and-DRL-Based-Resource-Allocation-for-V2X-Communications

利用DRL和GNN解决资源分配问题

# A. Traditional Methods

传统资源分配方法常采用博弈论、拍卖理论和进化算法进行研究。文献[43]研究了一种基于设备到设备通信的无线信道资源管理方法，将V2V通信中对时延和可靠性的要求转化为优化约束条件，这些约束可直接利用缓变信道信息进行计算。该研究将信道资源分配问题建模为优化问题，并采用启发式算法进行求解。文献[44]将车辆业务接入类型划分为安全类与非安全类车辆用户设备，以系统总吞吐量最大化为目标。文献[45]研究了非授权频谱中C-V2X用户与车辆自组织网络用户的共存问题，提出基于能量检测的频谱共享方案以降低两类用户间的冲突，其中运用匹配理论研究无线资源分配问题，并给出了车辆动态资源分配算法。文献[46]在考虑非完美信道状态信息的前提下，推导了单个VUE遍历容量的精确表达式，采用模拟退火算法获得较优的功率分配结果。文献[47]基于802.11p与C-V2X技术提出了低时延V2V通信资源分配方案，在混合架构中利用蜂窝基站进行信道选择决策，将时延最小化问题建模为最大加权独立集问题，提出了贪婪V2V链路选择算法并推导了理论性能下界。

在文献[48]中，提出了一种多状态协作方法。通过几何与网络分解方法，以逐状态方式在节点间进行能量分配。通过优化各状态的持续时间以实现节点效用最大化。文献[49]提出了一种基于分组传输的正交频分复用(OFDM)系统跨层自适应资源分配算法。该算法综合考虑随机业务到达与用户公平性问题，有效提升了系统频谱效率并改善了队列性能。文献[50]研究了具有协作组的无线供能通信网络，采用半定松弛方法联合优化时间分配、波束成形向量和功率分配，在满足两个通信组可用功率与服务质量的约束条件下，确保每个问题均获得全局最优解。文献[51]针对传感器网络中移动控制中心通过射频信号为传感器供电并采集信息的场景，提出了两种提升能效的解决方案。文献[52]探讨了基于网络编码的多播路由问题，旨在实现自组织网络中的最大流多播路由，并分析了编码节点的统计特性

基于随机图理论对自组织网络中的最大流进行了分析。文献[53]考虑了节点移动性，提出了一种能够跟踪动态拓扑变化的方法。该方法允许路径交叉，显著提升了端到端延迟和数据包投递率的性能。

# B. Methods Based on DRL and GNN

近年来，深度强化学习与图神经网络在资源分配领域的应用日益广泛。文献[54]首次将强化学习引入资源分配领域，提出分布式决策方案。文献[55]采用深度强化学习研究上行非正交多址接入系统的资源分配，设计离散化多深度Q学习结构以降低输出维度，同时引入深度确定性策略梯度算法实现连续功率分配的决策。文献[56]考虑用户隐私保护，将联邦学习引入资源分配：在较大时间尺度上通过谱聚类将信道条件相似的车辆分组，组内车辆通过联邦学习训练模型；在较长时间尺度上，车辆将模型上传至中心节点进行聚合后分发给各车辆。文献[57]在Actor-Critic框架中结合DQN与DDPG方法，使用DQN进行离散信道选择，DDPG进行连续功率选择，最后通过元强化学习获取网络优良初始化方式，增强其在动态变化环境中的快速适应能力。文献[58]采用图嵌入方法进行D2D网络链路调度，利用图表征学习提取节点特征以进行功率选择，并分析有监督与无监督训练对网络性能的影响。文献[59]将D2D通信链路视为图中节点，链路间干扰表示为边，把无线网络建模为有向图进行联合信道与功率分配，仅需少量样本即可获得接近最优的结果，且执行时间仅需数毫秒。文献[60]构建异质图并采用异质图神经网络学习策略，为深度神经网络提供信道与功率选择的先验知识，降低环境变化需重新训练深度神经网络时的训练复杂度，同时提出参数共享策略确保图神经网络所学先验知识有利于深度神经网络决策。

总而言之，尽管现有研究大多聚焦于优化网络架构以实现更高效的资源分配策略，却明显缺乏对丰富车辆本地观测信息的考量。这一研究空白促使我们开展此项研究。

# III. 系统模型

在本节中，我们将依次介绍系统的基本结构、干涉 $\{\mathbf{v}^{*}\}$ 的计算方法

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/54b9ef63520e965aa185c5e794c383243636cccdda913a27cac74310da2a2d21.jpg)  
图1：系统模型。

本文旨在通过证据、资源选择来阐明所提出的系统模型。

# A. Basic Structure

本文主要关注V2X范式中的V2I和V2V通信。如图1所示，我们研究交叉路口的车辆交通模型，其中基站（BS）被战略性地布置在交叉路口中心。车辆以随机分布方式进入道路，并在预设速度范围内行驶；每辆车随机选择速度并保持恒定速率。在该环境中，车辆通过V2I链路与基站通信，以传输高速率的娱乐与生活信息。此外，车辆利用V2V链路进行相互通信，在行驶过程中传输需要高可靠性的关键安全信息。

# B. Interference Calculation Method

假设有  $m$  个蜂窝用户（CUEs）与基站通信，表示为 $M = [1,2,3,\dots,m]$  ，和  $k$  对V2V用户（VUEs），表示为 $K = [1,2,3,\dots,k]$  。考虑到上行链路资源的相对稀疏利用，为了进一步优化频谱效率，我们假设V2V和V2I链路共享正交分配的上行链路频谱[61]。因此，第  $i$  个CUE的信号与干扰加噪声比（SINR）可以表示为：

$$
\gamma^ {c} [ i ] = \frac {P _ {i} ^ {c} h _ {i}}{\sigma^ {2} + \sum_ {j \in K} \rho_ {j} [ i ] P _ {j} ^ {v} \tilde {h} _ {j}} \tag {1}
$$

其中  $\sum_{j\in K}\rho_j[i]P_j^v\tilde{h}_j$  表示使用同一信道的VUE对第  $i$  个CUE造成的干扰。在此情况下，  $\rho_{j}[i] =$  等于1表明第  $j$  个VUE与第  $i$  个CUE存在同频干扰关系。

使用同一信道，而  $\rho_{j}[i] =$  否则为0。  $P_{j}^{v}$  表示第  $j$  个VUE的发射功率，  $\tilde{h}_j$  表示第  $j$  个VUE的功率增益。  $\sigma^2$  表示噪声功率，  $P_{i}^{c}$  表示第  $i$  个CUE的发射功率，  $h_i$  表示信道增益。根据香农公式，第  $i$  个CUE的通信容量可表示为：

$$
C ^ {c} [ i ] = B \cdot \log \left(1 + \gamma^ {c} [ i ]\right) \tag {2}
$$

其中  $B$  表示信道带宽。

类似地，第  $j$  个VUE的信干噪比可以表示为：

$$
\gamma^ {v} [ j ] = \frac {P _ {j} ^ {v} \cdot g _ {j}}{\sigma^ {2} + G _ {V 2 I} + G _ {V 2 V}} \tag {3}
$$

其中：

$$
G _ {V 2 I} = \sum_ {i \in M} \rho_ {j} [ i ] P _ {i} ^ {c} g _ {i, j} \tag {4}
$$

表示来自V2I链路在同一资源块内的干扰和

$$
G _ {V 2 V} = \sum_ {i \in M} \sum_ {j ^ {\prime} \in K, j \neq j ^ {\prime}} \rho_ {j} [ i ] \rho_ {j ^ {\prime}} [ i ] P _ {j ^ {\prime}} ^ {v} g _ {j ^ {\prime}, j} ^ {v} \tag {5}
$$

表示同一资源块内其他V2V链路的干扰，其中  $g_{j}$  表示第  $j$  个VUE的功率增益，而  $g_{i,j}$  和  $g_{j',j}^{v}$  分别表示来自第  $i$  个CUE和第  $j'$  个VUE的干扰功率增益。第  $j$  个VUE的信道容量可以表示如下：

$$
C ^ {v} [ j ] = B \cdot \log \left(1 + \gamma^ {v} [ j ]\right) \tag {6}
$$

# C. Resource Selection Method

在本文中，我们将车辆的资源配置细化为信道选择与功率级别选择。

- 在车联网通信系统中，信道资源的高效利用是实现效能最大化的先决条件。必须避免某些信道拥塞而其他信道闲置的情况。尤其在车辆自主选择信道资源的场景下，车辆不仅需要考虑自身信息传输的成功率，还需考量在复杂通信环境下是否占用了其他车辆唯一可用的传输途径。因此，仅基于各车辆自身采集的数据为其选择最优资源无法实现全局优化，整体效益才是关键所在。  
- 功率级别的选择通常发生在信道资源分配之后。较小的发射功率能减少对其他链路的干扰，但由于所选链路上的信噪比与干扰比较低，可能导致传输失败。反之，选择较大的发射功率会对其他链路造成更强干扰，同时增加能耗。因此，选择适当的发射功率级别对系统整体性能至关重要。

鉴于V2I链路的资源通常可由基站直接分配，我们考虑采用分布式算法来完成V2X资源分配任务。

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/626bea01153c06499e925939132bf5bead3bcdc7609acca539665382bc5f29a9.jpg)  
图2：车辆网络图构建

在预定的V2I资源分配条件下，假设子信道数量等于V2I链路数量，记为  $m$  ，且V2V通信功率列表中的功率级别数量为  $n$  。每个V2V链路共有  $m \cdot n$  个资源选择选项。目标是在满足V2V链路时延要求和可靠性的同时，最小化对V2I链路的干扰，从而最大化V2I链路的传输速率。为更好评估V2V链路性能，我们将V2V链路的可靠性要求转化为中断概率[43]。

模型与环境之间的交互过程可以描述如下：

在更大的时间尺度上，车辆根据距离关系确定邻居信息，隐式构建出拓扑图。在较小时间尺度上，车辆获取环境局部观测数据并收集邻居传输的信息，通过模型聚合这些输入以获得封装全局信息的低维特征。随后，运用深度强化学习（DRL）习得的策略进行信道选择与传输功率调整。需要强调的是，虽然车辆是系统模型的参与者，但在算法中V2V链路作为图谱节点和DRL框架中的智能体存在。最终由算法决策生成的动作将由车辆本体执行。这种去中心化方法通过局部交互与学习策略优化车载网络的资源分配。

# IV. 图神经网络模型的设计

# A. Graph Construction

为了构建车载网络的图表示，我们将每个V2V对视为一个节点，表示为  $N_{g} = [v_{1}, v_{2}, v_{3}, \dots, v_{k}]$ ，并将链路间的干扰关系视为边。对于节点  $v$ ，它包含一个初始特征向量  $x_{v}$  和一个存储其邻居节点索引的列表  $N(v)$ 。节点的初始特征封装了车辆对信道和干扰信息的本地观察。基于子信道数量等于CUE数量的假设，表示为  $m$ ，我们为第  $i$  个子信道记录V2V链路的瞬时信道功率增益，表示为  $G_{t}[i], i \in M$ ，V2I链路中从发射机到接收机的子信道功率增益，表示为  $H_{t}[i], i \in M$ ，以及各自的干扰信号强度

前一个时间槽，表示为  $I_{t - 1}[i], i \in M$ 。因此，节点  $v$  的特征可以表示为

$$
x _ {v} = \left\{G _ {t} \| H _ {t} \| I _ {t - 1} \right\} \tag {7}
$$

其中||表示向量的拼接。

现有方法主要将车载网络构建为完整图，以模拟所有链路间的相互干扰关系。然而当环境中车辆数量较大时，图结构会变得异常复杂，节点特征聚合的计算负载加剧，导致决策时间延长并可能引入额外延迟。为解决该问题，我们提出基于车辆间通信关系构建图结构。由于每个V2V链路包含发射车辆与接收车辆，假设环境中共有s辆车，表示为  $v = [V_{1}, V_{2}, V_{3}, \dots, V_{s}]$  。我们为每个V2V链路节点分配唯一标签，记为  $V_{t}$  to  $V_{r}$ ，其中  $V_{t}, V_{r} \in V$  。此处  $V_{t}$  代表发射车辆， $V_{r}$  代表接收车辆。图2展示了我们提出的基于通信关系的图构建方法。假设车辆  $V_{x}$  存在三个目标车辆： $V_{m}$ 、 $V_{p}$  和  $V_{o}$  。同时  $V_{x}$  也是车辆  $V_{y}$  和  $V_{z}$  的目标车辆。这些车辆构成的V2V链路节点标签分别为： $V_{x}$  至  $V_{m}$ 、 $V_{x}$  至  $V_{p}$ 、 $V_{x}$  至  $V_{o}$ 、 $V_{y}$  至  $V_{x}$  以及  $V_{z}$  至  $V_{x}$  。以节点  $V_{x}$  至  $V_{m}$  为例，我们将所有标签包含  $V_{x}$  或  $V_{m}$  的节点视为该节点的邻居，包括标签为  $V_{x}$  至  $Z$ 、 $Z$  至  $V_{x}$ 、 $V_{m}$  至  $Z$  和  $Z$  至  $V_{m}$  的所有节点，其中  $Z$  代表满足条件的任意车辆。

我们规定每辆车从附近的相邻车辆列表中选择三个传输目的地，这意味着每辆车会发起三个V2V链路。因此，环境中V2V链路的总数为3s，其中s代表车辆数量。对于每辆车而言，其相邻车辆的数量可近似为12辆。这一近似值基于以下事实：每辆车  $(V_{x}, V_{m})$  具有三条出向链路（ $V_{x}toZ, V_{m}toZ$ ）和不确定数量的入向链路（ $ZtoV_{x}, ZtoV_{m}$ ）。但由于链路总数是车辆数量的三倍，车辆作为接收端的平均次数也为3。因此，每辆车的邻居数量约为12，且不随环境中车辆数量的增加而变化。相比之下，在完全图中每个节点的邻居数量为  $3s - 1$ ，该值会随车辆数量持续增长，导致系统复杂度随车辆规模扩大而提升。

总之，我们提出的基于通信关系的图构建方法能将每个节点的邻居数量限制在相对稳定的范围。虽然该方法会忽略部分链路间的干扰关系，但本质上仍能保留来自邻居的最强干扰。当车辆密度较高时，仍然能以较小的计算负载提取图中的特征。此外，由于特征

在图中逐层传播，即使节点之间没有直接连接，经过多次迭代后，它们仍会受到远端节点的影响。此外，选择基于通信关系而非物理距离构建图的原因在于两个方面：一方面，这种方法直观反映了链路间的通信关系，节点之间的边反映了链路间的干扰；另一方面，该方法降低了车辆收集决策所需数据时的通信需求，从而有利于在车辆间隐式构建图。若基于物理距离构建图，由于车辆移动导致图的拓扑结构变化过快，这会给精确维护图结构带来较高复杂度，同时图中物理距离偏差的变化也会导致不同链路干扰估计产生更多偏差。对于由  $V_{x} \rightarrow Z$  和  $V_{m} \rightarrow Z$  表示的六个相邻节点，车辆  $V_{x}$  和  $V_{m}$  拥有所有相关信息。至于由  $Z \rightarrow V_{x}$  和  $Z \rightarrow V_{m}$  表示的相邻节点，在向  $V_{x}$  和  $V_{m}$  传输时，它们可以携带节点特征及其他相关信息。因此，通过在  $V_{x}$  和  $V_{m}$  之间建立双向连接，信息可以在图中传输而不会产生额外的通信开销。

我们对完整图与非完整图场景下图网络的计算负荷进行了对比分析。在环境中存在s辆车的情况下，该图包含3s个节点。假设每个节点具有  $D_{input} = 60$  的特征维度和 $D_{output} = 20$  的输出维度，并考虑单层聚合模型。每个节点的邻居数量为  $N_{nei}$  ，因此所需乘法运算的计算公式如下：

$$
N = D _ {\text {i n p u t}} \cdot D _ {\text {o u t p u t}} \cdot 3 s \cdot N _ {\text {n e i}} \tag {8}
$$

在构建完整图时， $N_{nei} = 3s - 1$ ，而采用我们提出的图构建方法时， $N_{nei} = 12$ 。本质上，这两种图构建方法之间乘法操作次数的差异取决于邻居节点的数量，其计算负载之比也等于邻居节点数量之比。因此，我们提出的方法中邻居节点数量不随车辆数量增加的特性显得尤为重要。

为了更好地表征车载网络环境，我们为图中的每条边分配一个权重。通常，距离较远的链路对当前链路的干扰较小。因此，我们记录发射机之间的距离，表示为一个方阵  $D = [d_{11}, d_{12}, \dots, d_{1s}; d_{21}, d_{22}, \dots, d_{2s}; \dots; d_{s1}, d_{s2}, \dots, d_{ss}]$  。假设链路  $V_{m}$  到  $V_{n}$  和  $V_{x}$  到  $V_{y}$  之间的干扰对应于节点  $v_{p}$  和  $v_{q}$  之间的边，其权重可以表示为：

$$
\delta_ {p q} = 1 - \frac {d _ {m x}}{\operatorname* {m a x} (D)} \tag {9}
$$

# B. GNN Network

在构建图之后，我们将介绍图神经网络模型。由于车载网络中车辆数量及车辆间通信关系频繁变化，传统GNN模型难以适应节点数量与邻接矩阵的快速变更，需要频繁调整图结构（如剪枝或添加边），甚至需要重新训练模型。因此我们引入擅长处理动态大规模图结构的GraphSAGE模型，并通过在节点特征聚合过程中加入边权重来增强其性能。

GraphSAGE模型的操作可分为三个主要步骤：邻居采样、特征聚合和特征更新。邻居采样适用于大规模图处理，其中各层每个节点需要采样的邻居数量是预定义的，允许随机选择这些邻居。这种随机采样机制限制了每个节点特征聚合的计算需求，确保即使面对极其复杂的图结构，模型仍保持计算可行性。尽管邻居采样函数和我们提出的图构建方法都用于管理计算资源消耗，但我们的方法不同于在完整图中对所有节点进行随机采样。在我们构建的图中，每层采样的邻居被限制在特定范围内，并且它们对当前节点的干扰随着层深的增加而减小。这种层次化的干扰递减有助于提取更优特征，因为第二层邻居对当前节点的影响小于第一层邻居。

假设采样邻居数量为  $S$ ，对于节点  $v$ ，其采样邻居集合记为  $N_{s}(v)$ ，聚合后的节点特征表示为  $z_{v}$ 。特征聚合过程可以表示为如下形式：

$$
z _ {v} = f _ {\text {a g g r e g a t e}} \left(\left\{x _ {u} \mid u \in N _ {s} (v) \right\}\right) \tag {10}
$$

其中， $f_{aggregate}$  表示模型的聚合函数。我们采用均值聚合函数，但存在一个关键区别：我们引入边权重以反映各相邻节点特征的重要性。这可以表示为：

$$
z _ {v} = \sigma \left(W _ {a} \cdot \sum_ {u \in N _ {s} (v)} \frac {X _ {u} \cdot \delta_ {u v}}{\left| N _ {s} (v) \right|} + b _ {a}\right) \tag {11}
$$

其中， $W_{a}$  表示聚合函数的权重， $b_{a}$  表示聚合函数的偏置项， $\sigma$  表示激活函数。

在特征更新过程中，我们将邻居特征的聚合结果与节点自身的初始特征相结合，得到最终的节点特征嵌入：

$$
h _ {v} = f _ {\text {u p d a t e}} \left(z _ {v}, x _ {v}\right) \tag {12}
$$

在该方法中，我们选择求和作为将邻居特征的聚合结果与初始节点特征相整合的手段，随后对这一整合结果进行处理。

Algorithm 1: GraphSAGE  
1 Input: Graph network model, Node features;  
2 Output: Aggregation result;  
3 Define: Maximum iteration count max_iter and convergence threshold  $\epsilon$ ;  
4 Initialize: Initialize model, iter  $\leftarrow 0$  converged  $\leftarrow$  False;  
5 Large Time Scale: Construct global graph;  
6 Small Time Scale: Update and Aggregate Node Features;  
7 while iter  $<$  max_iter and converged  $=$  False do  
8 for each node  $v$  from  $N_{g}$  do  
9 Node Neighbor Sampling:  
10 Randomly select five neighbors of node  $v$  and store them in  $N_{s}(v)$   
11 for each neighbor  $u \in N_{s}(v)$  do  
12 Randomly select five neighbors of node  $u$  and store them in  $N_{s}(u)$   
13 end  
14 Feature Aggregation and Update:  
15 for each neighbor  $u \in N_{s}(v)$  do  
// Update feature of neighbor  $u$   
16  $h_u = f_{update}(x_u, f_{aggregate}(x_n, n \in N_s(u)))$   
17 end  
// Update feature of node  $v$   
18  $h_v = f_{update}(x_v, f_{aggregate}(h_u, u \in N_s(v)))$   
19 end  
// Check for convergence  
20 converged  $\leftarrow$  check_convergence()  
iter  $\leftarrow$  iter + 1  
21 end

通过使用训练得到的权重参数进行特征提取，得到的节点嵌入表示为  $h_{v_0}$ 。因此，更新函数  $f_{update}$  可以表示如下：

$$
f _ {\text {u p d a t e}} \left(z _ {v}, x _ {v}\right) = \sigma \left(W _ {u} \cdot \left(z _ {v} + x _ {v}\right) + b _ {u}\right) \tag {13}
$$

其中， $W_{u}$  和  $b_{u}$  表示特征更新函数中训练后的权重和偏置向量。因此，单轮的完整聚合过程可以表示为：

$$
h _ {v} = f _ {\text {u p d a t e}} \left(x _ {v}, f _ {\text {a g g r e g a t e}} \left(\left\{x _ {u} \mid u \in N _ {s} (v) \right\}\right)\right) \tag {14}
$$

类似地，两层聚合过程可以表示为：

$$
h _ {v} = f _ {\text {u p d a t e}} \left(x _ {v}, f _ {\text {a g g r e g a t e}} \left(\left\{f _ {\text {u p d a t e}} \left(x _ {u}, \right.\right.\right.\right.\left. \right. \tag {15}
$$

$$
f _ {a g g r e g a t e} \left(\left\{x _ {n} | n \in N _ {s} (u) \right\}\right) \left| u \in N _ {s} (v) \right\})
$$

算法1总结了GraphSAGE模型的特征提取过程。

为确保GraphSAGE模型提取的特征有利于智能体的决策，我们为其设计了独特的更新机制。由于GraphSAGE模型采用全局部署，而DDQN

由于该模型仅影响单个智能体，将两个网络整合进行联合训练具有挑战性。因此，我们选择分别训练GraphSAGE模型。

图神经网络的训练通常分为监督学习、半监督学习或无监督学习。无监督学习在训练阶段可能产生显著的计算开销，而监督学习和半监督学习则受限于获取节点标签的成本。因此，我们致力于寻找一种成本效益高的方法，既能为网络提供学习方向，又能增强DDQN模型的决策能力。

事实上，我们利用GraphSAGE模型的关键作用来聚合节点特征，帮助智能体准确辨别每个信道的质量。DDQN网络采取行动后获得的奖励为信道提供了可靠评估。因此，系统可将智能体为每个子信道获取的奖励信息存储在对应子信道数量的矩阵  $R_{g} = [r_{1}, r_{2}, r_{3}, \dots, r_{m}]$  中，该矩阵作为相应节点的标签。

然而，存储的奖励值存在一定延迟，使其当前不适合作为绝对标签。但它们仍能为复杂环境中的网络学习提供模糊方向指导。为解决这一问题并尝试高效利用还原后的奖励值，我们提出了一种更温和的更新策略。该策略通过滞后网络聚合节点特征，并根据特定比例用标签对它们进行加权。这种方法主要有两个优势：

1）它削弱了标签的绝对影响，保持了学习过程的稳定性。2）它逐步引导网络的预测值趋近于理想标签值。

通过采用这种适度更新策略，可以在学习稳定性与方向性指导之间取得平衡，最终提升DDQN模型基于GraphSAGE聚合特征进行决策的性能。

具体而言，这里采用均方误差（MSE）函数作为网络的损失函数，其定义如下：

$$
\operatorname {L o s s} (\theta) = \sum_ {v \in N _ {g}} \left(y _ {v} - h _ {v}\right) ^ {2} \tag {16}
$$

其中：

$$
y _ {v} = \kappa h _ {v} ^ {o l d} + (1 - \kappa) R _ {g} ^ {v} \tag {17}
$$

其中  $\theta$  代表Graph-SAGE模型中的权重参数,  $y_{v}$  表示用于网络更新的平滑标签,  $\kappa$  指代聚合结果的权重因子,  $R_{g}^{v}$  表示节点  $v$  的标签, 而  $h_{v}^{old}$  代表来自滞后网络的聚合结果。

# 五. GNN-DDQN模型用于资源分配问题

在本节中，我们主要介绍基于GNN-DDQN模型解决V2X通信资源分配问题的方法。首先给出强化学习的关键方程与核心原理，

DDQN模型的基础上，我们进一步探讨如何将其与之前提到的GraphSAGE模型相结合，以解决资源分配问题。

# A. Key Formulas in RL and DDQN

强化学习通常被构建为马尔可夫决策过程，表现为智能体与环境之间的交互。智能体从环境中获取状态信息，根据策略  $\pi$  做出决策，然后执行动作。环境则会向智能体提供奖励并更新当前状态。

具体而言，在每个时间步  $t$ ，智能体从环境的状态空间中获取当前状态  $s_t$ ，并根据现有策略  $\pi$  从动作空间  $A$  中选择合适的动作  $a_t$ ，以确定V2V链路的传输信道和传输功率。该策略  $\pi$  由状态-动作函数（也称为Q函数）决定，记为  $Q(s_t, a_t)$  。在深度强化学习中，通常使用深度学习来近似Q函数，以适应复杂的环境变化。智能体执行动作后，环境将转换到新状态  $s_{t+1}$ ，并根据智能体选择的结果提供奖励  $r_t$  以评估动作质量。本文中的奖励源自V2V链路的中断概率和V2I链路的速率。

接下来，我们将依次介绍DDQN网络中状态、动作和奖励的细节：

1) State space: 在我们考虑的V2X环境中, 真实状态信息主要包含车辆对环境的观测数据  $x_{v}$  以及通过Graph-SAGE模型从这些观测数据中提取的低维特征  $h_{v}$  。为辅助智能体做出更优决策, 每辆车会向目标车辆发送其信道选择信息。基于此, 我们收集每个智能体邻近车辆先前时隙的信道选择信息  $N_{t-1}$ , 计算车辆待发送剩余比特数与需发送总比特数的比值  $L_{t}$ , 以及延迟约束下的剩余传输时间  $U_{t}$  。综合这些信息, 智能体获取的状态可表示为:

$$
S _ {v} ^ {t} = \left\{h _ {v} ^ {t} \| x _ {v} ^ {t} \| N _ {t - 1} ^ {v} \| L _ {t} ^ {v} \| U _ {t} ^ {v} \right\} \tag {18}
$$

2) Action space: 根据收集和观测到的状态信息, DDQN网络依据策略  $\pi$  选择动作  $a_{t} \in A$  。由于智能体需要同时选择子信道和功率级别, 我们将这两类动作组合成复合动作。智能体选择的复合动作会映射到两个维度上, 分别代表子信道和功率级别的选择。本文考虑一个相对简单的场景, 包含三个功率级别和  $m$  个资源块, 因此总共存在  $3 \times m$  种可能的动作。假设智能体选择动作  $a_{t}$ , 我们将其分解如下:

$$
a _ {r} ^ {t} = a _ {t} \% m \tag{19}
$$

和

$$
a _ {p} ^ {t} = a _ {t} / m \tag {20}
$$

其中 $\%$ 表示模运算，/代表向下取整除法， $a_{r}^{t}$ 表示分解后的子信道选择操作， $a_{p}^{t}$ 代表功率等级的选择。

3）Reward function：强化学习的目标是使环境中的V2V链路在最大限度满足V2V通信低时延高可靠性需求的同时，尽可能减少对V2I链路的干扰，以最大化V2I链路的传输速率。先前，我们将V2V链路的时延和可靠性要求转化为对中断概率的要求。因此，在设计奖励函数时，我们只需考虑V2V链路的中断概率和V2I链路的传输速率。此外，我们还根据链路传输已耗费的时间设置了惩罚项。奖励函数的具体表达式如下：

$$
\begin{array}{l} r _ {t} = \lambda_ {c} \sum_ {i \in M} C ^ {c} [ i ] + (1 - \lambda_ {c}) \sum_ {j \in K} C ^ {v} [ j ] \tag {21} \\ - \lambda_ {p} \left(T _ {0} - U _ {t}\right) \\ \end{array}
$$

其中， $\lambda_{c}$ 代表V2I链路的权重， $\lambda_{p}$ 表示所用传输时间的权重。 $U_{t}$ 表示剩余时间， $T_{0}$ 则代表传输延迟限制。因此（ $T_{0} - U_{t}$ ）表示用于传输的时间。智能体的长期折扣回报可表示为：

$$
R _ {t} = E \left[ \sum_ {n = 0} ^ {\infty} \beta^ {n} r _ {t + n} \right] \tag {22}
$$

其中， $\beta \in [0,1]$  表示奖励的折扣因子。 $\beta$  越大表明智能体具有更长远的视角，而较小的  $\beta$  则意味着智能体更关注即时奖励。

在深度强化学习中，主要目标是学习一个最优策略  $\pi^{*}$ ，该策略最大化长期折扣奖励，以及开发一个深度网络模型，该模型可以预测与状态-动作对对应的Q值。对于给定的状态-动作对  $(s_{t},a_{t})$ ，Q值  $Q(s_{t},a_{t})$  表示在根据策略  $\pi$  采取动作  $a_{t} \in A$  后获得的预期累积折扣奖励。因此，Q值可用于评估给定状态下动作的质量。一旦我们对Q值有了准确的估计，就可以根据以下方程选择动作：

$$
a _ {t} = \arg \max  _ {a \in A} Q \left(s _ {t}, a\right) \tag {23}
$$

这意味着我们选择具有最大Q值的动作。在DDQN中，对应于最优策略的Q值，表示为  $Q^{*}$ ，可以通过以下更新方程获得：

$$
\begin{array}{l} Q _ {n e w} \left(s _ {t}, a _ {t}\right) = Q _ {o l d} \left(s _ {t}, a _ {t}\right) + \alpha \left[ r _ {t + 1} + \right. \\ \beta Q _ {o l d} ^ {\text {t a r g e t}} \left(s _ {t + 1}, \arg \max  _ {a \in A} Q _ {o l d} \left(s _ {t + 1}, a\right)\right) \tag {24} \\ \left. - Q _ {o l d} \left(s _ {t}, a _ {t}\right) \right] \\ \end{array}
$$

其中， $\alpha$  是学习率，而等式右侧的第二项是时间差分。

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/106cdbe13dfa90a1bdfeea8ca479c7bcc032729cec94bba166797d612f670207.jpg)  
图3：GNN-DDQN框架。

TD误差用于更新Q值。折扣因子由γ表示。 $Q_{\mathrm{old}}$  表示由Q网络预测的Q值，而 $Q_{\mathrm{old}}^{\mathrm{target}}$ 表示由目标Q网络预测的Q值。

假设 Q 网络的权重由  $\phi$  表示，当输入是智能体观察到的状态时，输出将是每个动作对应的 Q 值。我们使用 TD 误差来优化 Q 网络的参数  $\phi$ ，可以表示如下：

$$
L o s s (\varphi) = \sum_ {\left(s _ {t}, a _ {t}\right) \in D} \left(y _ {Q} - Q \left(s _ {t}, a _ {t}, \varphi\right)\right) ^ {2} \tag {25}
$$

其中， $D$  表示状态-动作对的集合， $y_{Q}$  代表期望目标Q值，该值可由以下公式给出：

$$
y _ {Q} = r _ {t} + \beta Q _ {\text {o l d}} ^ {\text {t a r g e t}} \left(s _ {t + 1}, \arg \max  _ {a \in A} Q _ {\text {o l d}} \left(s _ {t + 1}, a\right), \varphi^ {\prime}\right) \tag {26}
$$

其中， $\varphi'$  表示目标Q网络的参数。

# B. GNN-DDQN Framework and Training-Testing Process

图3详细展示了该算法的结构和操作步骤。GNN-DDQN模型的运行可分为三个步骤：构建图结构、提取全局信息的低维特征，以及基于Q网络预测的Q值做出决策。需要注意的是，GraphSAGE模型的训练过程可参考先前介绍的内容。此处我们将重点关注DDQN模型的训练以及GNN-DDQN模型的测试。

在强化学习中, 智能体所需的所有数据都是通过智能体与环境之间的交互生成的。我们使用元组  $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$  作为单个

训练样本并将其存储在回放缓冲区中，定期从中抽取样本来更新DDQN网络。环境模拟器包含VUE、CUE及其信道模型。环境中的车辆按照均匀分布随机放置在交叉口模型中。根据车辆间的距离关系，建立V2V和V2I通信链路。智能体基于环境中的CSI选择信道和发射功率。随后环境会相应更新衰落信息和干扰信息，生成新状态 $s_{t+1}$ 。同时，智能体会根据所采取动作的质量获得奖励 $r_t$ 。在训练阶段，我们采用 $\epsilon$ -贪婪策略来平衡探索与利用。在测试阶段，根据预测的Q值选择动作，具体是选择具有最高Q值的动作。

在初始化阶段，智能体基于车联网通信链路识别邻近车辆，在宏观时间尺度上隐式构建出以距离为边权重的拓扑图。在微观时间尺度上，智能体采集局部观测值  $S_{v}^{t}$ ，并将其与邻近智能体的特征进行聚合以实现特征更新，最终生成低维表征  $h_{v}$ 。该表征与局部观测值  $S_{v}^{t}$  共同构成智能体状态。随后智能体通过Q网络预测具有最高Q值的动作，并从环境获取奖励。训练过程中，针对每个动作预测的Q值将作为基线评估值存储，经软化处理后得到  $y_{v}$ ，用于指导图神经网络模型的更新。智能体通过目标Q网络预测的Q值优化策略，并同步更新Q网络本身。在测试阶段，直接执行Q网络预测的最大Q值对应动作作为最终决策。

算法2详细说明了GNN-DDQN模型的训练过程，而算法3则展示了该模型的测试流程。

# 第六部分 仿真结果与分析

# A. Simulation settings

本研究中的仿真采用Python 3.8和TensorFlow 2.6实现，所采用设置与文献[54]中的假设相似。我们考虑在2 GHz载波频率下运行的单蜂窝系统。仿真遵循3GPP TR 36.885[62]所述的曼哈顿场景配置，涵盖视距(LOS)与非视距(NLOS)信道条件。车辆分布被建模为空间泊松过程，随机分布在每条车道上。假设每辆车与其三个最近邻车辆建立V2V通信链路，使得V2V链路数量为车辆数量的三倍。

我们采用深度为2的GraphSAGE模型，每层采样5个邻居节点，并融合节点自身特征进行更新。图网络提取的特征维度设定为20，每个节点输入图网络的特征维度为60，并采用平均聚合方式。

Algorithm 2: Training Process for GNN-DDQN  
Input: GraphSAGE model, Q-network model, Simulation environment  
Output: Q-network, GraphSAGE-network

1 Define: Maximum iteration count max_iter and convergence threshold  $\epsilon$ ;

2 Initialize: Randomly initialize policy  $\pi$ , initialize model, start environment simulation and add vehicles, CUEs, and VUEs, iter  $\leftarrow 0$ , converged  $\leftarrow$  False;

3 Large Time Scale: Construct global graph;

4 Small Time Scale: Joint GNN-DDQN model selects actions;

6 Execute Algorithm 1  
5 while iter  $<$  max_iter and converged  $=$  False do // Execute feature extraction and aggregation

// Run the DDQN Network  
7 For node  $v$ , the agent selects subchannels and transmission power based on policy  $\pi$  using  $h_v$  (Eq. 18) and locally observed states  
8 The environment simulator generates a new state  $s_{t+1}$  and reward  $r_t$  based on the agent's actions (Eq. 21)

9 Store the selected subchannels and corresponding rewards in  $R_{q}$

10 Collect and store the tuple  $(s_t, a_t, r_t, s_{t+1})$  in the experience replay buffer

11 Sample a mini-batch from the experience replay buffer

12 Train the DDQN network using the mini-batch (Eq. 24 and Eq. 25)

13 Update the GNN network using labels derived from  $R_{g}$  // Update policy  $\pi$

14 Update policy  $\pi$  : select the action with the highest Q-value (Eq. 23)

15 converged  $\leftarrow$  check_convergence() iter  $\leftarrow$  iter + 1

该函数被采用。对于DDQN模型，输入状态维度为102，采用三层神经网络模型，每层分别包含500、250和120个神经元。最终输出60维动作的Q值。层间使用线性整流函数（ReLU）作为非线性激活函数。该激活函数的表达式如下：

$$
f _ {R e l u} (x) = \max  (0, x) \tag {27}
$$

学习率设定为在训练期间逐步递减。图网络的初始学习率选为0.01，DDQN网络的初始学习率选为0.005，两者最低值均为0.0001。更详细的参数设置见表I。

Algorithm 3: Testing Process for GNN-DDQN  
Input: Trained Q-network model, Trained GraphSAGE model, Simulation environment  
Output: Evaluation results

1 Define: Maximum iteration count max_iter;

2 Initialize: Load the Q-network model and GraphSAGE model, start the environment simulation, and add vehicles, CUEs, and VUEs, iter  $\leftarrow 0$ ;

3 Large Time Scale: Construct global graph;

4 Small Time Scale: Joint GNN-DDQN model selects actions;

5 while iter  $<$  max_iter do // Execute feature extraction and aggregation

6 Execute Algorithm 1  
// Run the DDQN Network For node  $v$  , the agent selects the action with the highest Q-value based on  $h_v$  (Eq. 18) and locally observed states

8 The environment simulator updates the environment based on the agent's actions (Eq. 21)

9 Update the evaluation results, which include the average V2I capacity and the success rate of V2V communication 10 iter  $\leftarrow$  iter  $+1$

表 1: 系统参数  

<table><tr><td colspan="2">Parameters of System Model</td></tr><tr><td>Description</td><td>Specification</td></tr><tr><td>Carrier Frequency</td><td>2 GHz</td></tr><tr><td>Height of Vehicle Antenna</td><td>1.5 meters</td></tr><tr><td>Bandwidth of Single Subchannel</td><td>1.5 MHz</td></tr><tr><td>Gain of Vehicle Antenna</td><td>3 dBi</td></tr><tr><td>Height of Base Station Antenna</td><td>25 meters</td></tr><tr><td>Noise Figure of Vehicle Receiver</td><td>9 dB</td></tr><tr><td>Gain of Base Station Antenna</td><td>8 dBi</td></tr><tr><td>Vehicle Speed</td><td>36 km/h to 54 km/h</td></tr><tr><td>Noise Figure of Base Station Receiver</td><td>5 dB</td></tr><tr><td>Distance Threshold for Neighbor Vehicles</td><td>150 meters</td></tr><tr><td>Number of Lanes in Environment</td><td>4 per direction, total 16 lanes</td></tr><tr><td>Maximum Delay for V2V Link</td><td>100 ms</td></tr><tr><td>Transmission Power Levels</td><td>[23, 10, 5] dBm</td></tr><tr><td>Noise Power</td><td>-114 dBm</td></tr><tr><td>Weight Coefficients [λc, λp]</td><td>[0.3, 1]</td></tr><tr><td>First Layer Neighbor Sampling Count</td><td>5</td></tr><tr><td>Second Layer Neighbor Sampling Count</td><td>5</td></tr><tr><td>Depth of GraphSAGE</td><td>2</td></tr><tr><td>Dimension of Node Feature Output</td><td>20</td></tr></table>

注意：诸如“载波频率”、“车载天线高度”、“单子信道带宽”、“车载天线增益”、“基站天线高度”等参数均改编自[54]。

# B. Model Training Status

为了分析网络的训练过程，我们监控了训练进度。由于我们的GraphSAGE模型和DDQN网络是分开训练的，我们将分别分析这两个网络的训练性能。

1) 图SAGE模型的训练损失：图4展示了图SAGE模型在5000次训练迭代中的损失表现，其中图SAGE模型是

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/c87b46d8f1f4dd3fb4a99707ef323c3d8946e9997578df50864ded8a8967c5c4.jpg)  
图4: GraphSAGE的训练损失

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/8050ce9a9fdf4ff2c10945eb2e4c0ee5ddf68c7d89ec1dbb21165fdd2f9d363b.jpg)  
图6：平均V2I速率与车辆数量的关系

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/81e59b1575710ec7f0072e81feff508633c1df2c5173ccb93954da6c59d47543.jpg)  
图5：GNN-DDQN的训练效果。

每50次迭代更新120次。可以观察到损失迅速收敛。由于每2000次迭代环境会重置，每次环境重置时损失会出现显著波动。然而随着迭代次数增加，环境重置造成的波动逐渐减弱，模型开始适应不同环境。

2）GNN-DDQN网络在不同训练迭代次数下的性能对比：图5展示了经过10,000次训练迭代后在仿真环境中获得策略的测试性能。实际应用中，为获得收敛策略，我们进行了近40,000次训练迭代。此处呈现的是前10,000次训练迭代的结果。可以观察到，随着训练迭代次数的增加，模型测试中的评估V2I通信速率和平均V2V通信成功率逐渐提升，但增长幅度逐步放缓。

# C. Static Environment Testing

我们使用三种基线方法进行比较。第一种是随机资源分配方法，智能体随机选择信道和功率水平，作为性能下限。第二种是[63]中提及的方法，该方法将信道条件相似的车辆分组，并迭代分配子信道。第三种是[54]采用的方法，使用通用DQN模型进行资源分配，这有助于我们分析引入GNN带来的性能增益。

为了解决智能体同时选择信道时因信息延迟导致的资源冲突问题，我们将所有智能体的决策过程划分为十批进行处理。一旦智能体完成决策，它们将保持其行动固定不变，直到下一轮重新分配开始。为了获得更具泛化能力的策略，在训练阶段我们会定期重置环境状态。

在获取测试结果并绘制图表时，我们将环境重置100次，对每个环境取200个样本的平均值，然后在100个环境中对数据再次取平均值。我们观察到智能体学习到的策略在变化环境中会呈现波动，但波动范围是可控的。考虑到车辆密度等因素在不同环境中可能存在显著差异，并不存在恒定的最优解，因此出现波动是合理的。

1) V2I通信速率：图6展示了所有子信道的总V2I速率与车辆数量之间的关系。从图中可以明显看出，随着车辆数量的增加，环境中V2V链路的数量也随之上升。这导致V2V通信链路对V2I链路产生的干扰加剧，进而造成V2I链路速率下降。虽然DQN方法和我们提出的GNN-DDQN方法都基于强化学习，但GNN的引入使智能体能够获取更丰富、更全面的

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/d67230230a381ffa186dd08dd40a633d8f6c10628e51f921a656aa763e59ceae.jpg)  
图7: 平均V2V成功率与车辆数量之间的关系。

全面的全局状态信息，从而带来卓越的性能。基于传统方法和随机策略的方法会受到特别严重的干扰，因为强化学习方法能够学习最大化长期奖励的策略，因而在高密度通信环境中表现更佳。

2)V2V通信成功率：图7展示了环境中所有V2V链路的传输成功率与车辆数量之间的关系。如前所述，环境中V2V链路的数量是车辆数量的三倍。可以观察到，随着车辆数量的增加，整体通信成功率呈现下降趋势，这符合一般认知。采用随机策略的V2V通信成功率下降速度最快，其他方法的下降速率较慢，表明这些方法具有提升资源利用效率的能力。显然，我们提出的GNN-DDQN方法优于传统DQN网络，进一步证明GNN能够辅助DRL提升系统性能。

# D. Strategy Analysis

如图8所示，为分析GNN对智能体学习策略的影响，我们遵循[54]的方法，收集智能体在不同剩余传输时间下做出的决策，并分析各功率级别被选择的统计概率。与[54]不同的是，当V2V链路传输失败时，我们对智能体施加了显著惩罚。因此，当剩余传输时间仅为0.01秒时，智能体倾向于选择更高功率级别。如图所示，当传输时间充足时，智能体倾向于选择较低功率级别以减少对V2I链路的干扰；而当传输时间紧张时，则倾向于选择更高功率以确保V2V链路传输成功。此外，在传输开始时，智能体同样

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/db989eef1de93e3dd377efb50c32a7e4dee5aab6e5f94d5019c55a4c1bd4c626.jpg)  
图8: 剩余时间与功率选择的关系

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/ebfba4a5f57d6e5f7defd4f91df7c62a22c14e91a067d3eac5aaa71116873d5a.jpg)  
图9：完全图与不完全图的单次决策时间对比。

倾向于选择更高的功率，这可能是因为智能体并非在每个时隙都重新选择信道，而初始设定较高的功率水平是一种审慎的选择。这表明智能体习得的策略捕捉到一定程度的经验，有助于分析策略的有效性。

图9展示了使用全连接图与非完整图构建GNN时的性能差异。当采用全连接图时，环境中的节点数量随车辆数量线性增长，导致所需训练时间略有增加。可以观察到，使用非完整图所需的时间基本保持恒定，而全连接图所需时间随着节点数量增加逐渐上升。为确保对比的可靠性，此处省略了非完整图场景的数据预处理步骤，因为非完整图需要额外步骤从图中获取邻居信息。我们未采用先进的高效API优化

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/7f400716d5b577e4765d08a22749112410db9d0f04d1c11ece9b49881ad81f62.jpg)  
图10：动态环境中车辆数量、V2I通信速率与V2V通信成功率的时间动态变化。

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-24/df4bf428-958e-4447-97bd-bf820dd2f0cd/072b71b4bb981465f512fb94a16dea5d3d1e5bf934e9e327ec8fb348af7000cd.jpg)  
图11：算法性能箱线图：五个时间段内的车辆数量、V2I通信速率及V2V通信成功率。

针对GNN的加速方法已被提出以优化该过程[64][65]；但我们仍采用原始代码实现。因此，在CPU上的处理可能耗费较长时间，包括从CPU向GPU传输数据造成的延迟。我们的比较主要聚焦于GPU上的计算速度。对于不完整图曲线，由于邻居节点数量较少且不随车辆数量增加，单次决策所需时间保持相对较小，且受环境中车辆数量的影响较小。

# E. Dynamic Environment Testing

最后，为验证我们提出方法在车辆数量动态变化环境中的性能，我们提供了动态条件下的测试结果。一方面，由于车辆分布随时间频繁变化，动态环境对算法的鲁棒性提出了更高要求；另一方面，环境中快速变化的信道占用情况要求算法具备更高且稳定的性能。为获取丰富的测试数据，

我们设计了一个使用字典存储车辆的动态环境，在车辆离开环境时删除对应车辆条目。每次更新车辆位置时，存在一定概率新增车辆；若未添加车辆，则该概率会递增。一旦成功添加车辆，概率将重置为零。为确保对更多车辆进行充分探索，我们采用概率模式控制车辆添加，保证环境中车辆数量持续稳定增长。

在动态环境下的决策过程中，环境中十分之一的车辆会在每个时间槽内做出决策。具体而言，车辆被划分为十个批次依次进行决策，每0.005秒完成一个批次的决策，这与我们先前提出的方法保持一致。

1) 动态环境中的车辆数量、瞬时V2I速率与瞬时V2V成功率：图10展示了车辆数量、V2V通信瞬时平均成功率及各信道瞬时V2I通信速率总和随时间的变化情况。车辆数量随时间持续增长。在环境初始阶段，V2V通信成功率存在显著振荡，这种非每次测试均会出现的波动源于环境随机性，并会快速趋于稳定。由图可见，V2V通信成功率保持高度稳定，不随车辆位置变化而波动，但会随着环境中车辆数量增加而逐渐降低。对于V2I速率，由于策略在静态环境中训练，无法完全适应动态环境。在保障V2V通信成功率的同时，未能有效最小化对V2I链路的干扰，导致V2I速率出现较大波动。总体而言，随着车辆数量增加，V2I速率呈现下降趋势。

2)结果分析：在图11中，抽样的5000个数据点被划分为五个时段，每个时段代表特定时间范围内的数据。针对各时段分别分析了车辆数量、V2I通信速率与V2V通信成功率的性能表现。在首个时段中，V2V通信成功率呈现出较高频率的异常值，这归因于从瞬时值观测到的环境振荡现象。尽管V2I通信速率偶现异常值，但始终处于可控范围内。总体而言，随着车辆数量逐步增加，V2I通信速率与V2V通信成功率均呈现渐次下降趋势。各时段中的数值保持相对稳定，异常值较少，印证了算法的鲁棒性。

# 七、结论

在本文中，我们从集中式资源分配方法优于分布式方法这一原因入手，并

尝试将图神经网络（GNN）与深度强化学习（DRL）相结合，使每个智能体能够从局部观测中获取更多信息。针对图神经网络通常部署在全局范围且难以适应动态图结构的问题，我们提出了一种特别适用于车辆网络的新型图构建方法。该方法无需增加额外通信开销即可隐式构建图网络，同时采用归纳式GraphSAGE模型来处理车辆数量的可变性。最后，我们将提出的GNN模型与双深度Q网络（DDQN）相结合来解决资源分配问题，并构建了具有固定车辆数量的静态环境和车辆数量变化的动态环境进行测试。仿真结果表明，我们提出的GNN模型能有效改进智能体学习到的策略，其性能优于独立的DQN方法。

根据理论分析和仿真结果，可以总结出以下结论：

- 本文提出的图构建方法确保了分布式网络部署，并减少了所需的决策时间。  
- GraphSAGE模型能够捕捉图中包含全局结构信息的特征。  
- 图神经网络的整合使智能体能够从局部观察中捕获更多信息，从而促进更优决策。

尽管所提出的方法具有优势，但仍存在改进空间。GraphSAGE模型采用归纳式学习方法获取通用聚合函数，这使其难以适应不同节点的个体特性，从而限制了模型性能。未来的工作可着重于增强模型架构，确保图神经网络在提取更精确状态特征的同时能够适应车辆数量的变化。这可能涉及开发更复杂的聚合函数，或将额外的上下文信息整合到模型中。通过解决这些挑战，所提出的方法可以实现更优性能，并进一步优化车辆网络中的资源分配。

# 参考文献

[1] A. Zanella, N. Bui, A. Castellani, L. Vangelista 与 M. Zorzi, 《智慧城市物联网》, IEEE Internet of Things Journal 第 1 卷第 1 期, 第 22-32 页, 2014 年。[2] Z. Feng, B. Wang, Z. Chang, T. Hämäläinen, Y. Zhao 与 F. Hu, 《基于可重构智能表面的车辆定位联合主动与被动波束成形》, 收录于 IEEE Transactions on Intelligent Transportation Systems, 2024 年。doi: 10.1109/TITS.2024.3408315。[3] M. Luan, B. Wang, Z. Chang, T. Hämäläinen 与 F. Hu, 《RIS 辅助集成传感与通信系统的鲁棒波束成形设计》, IEEE Transactions on Intelligent Transportation Systems 第 24 卷第 6 期, 第 6227-6243 页, 2023 年 6 月。doi: 10.1109/TITS.2023.3248145。[4] K. Abboud, H. A. Omar 与 W. Zhuang, 《车联网通信中 DSRC 与蜂窝网络技术的互联：综述》, IEEE  
Transactions on Vehicular Technology 第65卷第12期，第9457-9470页，2016年。[5]J.B.Kenney，《美国专用短程通信（DSRC）标准》，Proceedings of the IEEE第99卷第7期，第1162-1182页，2011年。

[6] 吴凡、吕飞、任杰等，《基于互联网卡用户画像特征的高效离网预测模型设计》，发表于IEEE Transactions on Mobile Computing，第23卷第2期，第1735-1752页，2024年2月。doi: 10.1109/TMC.2023.3241206。[7] 卢楠、程楠、张宁等，《网联车辆：解决方案与挑战》，  
IEEE Internet of Things Journal，第1卷第4期，第289-299页，2014年。[8] Siegel J E, Erb D C, Sarma S E，《网联车辆生态综述——架构、使能技术、应用与发展领域》，IEEE Transactions on Intelligent Transportation Systems，第19卷第8期，第2391-2406页，2018年。[9] 王健、刘军、加藤修，《自动驾驶中的网络与通信技术综述》，  
IEEE Communications Surveys &Tutorials，第21卷第2期，第1243-1274页，2019年。[10]罗国栋、邵春尧、程楠等，“EdgeCooper：面向增强车辆感知的网络感知协同LiDAR感知系统”，收录于  
IEEE Journal on Selected Areas in Communications，第42卷第1期，第207-222页，2024年1月。doi: 10.1109/JSAC.2023.3322764。[11] 王伟、程楠、李敏等，“价值导向：基于信息价值的CAVs资源调度新方法”，收录于IEEE Transactions on Vehicular Technology，第73卷第6期，第8720-8735页，2024年6月。doi: 10.1109/TVT.2024.3355119。[12] 邵志强、吴骞、范平等，《基于深度强化学习的5G-V2X异构网络语义感知资源分配》，IEEE Communications Letters，第1-1页，2024年。[13] 吴骞、王伟、范平等，《面向异构车联网边缘计算的URLLC感知资源分配》，  
IEEE Transactions on Vehicular Technology，第73卷第8期，第11789-11805页，2024年。[14] Garcia M H C、Molina-Galan A、Boban M等，《5GNR V2X通信技术详解》，IEEE Communications Surveys & Tutorials，第23卷第3期，第1972-2026页，2021年。[15] Sehla K、Nguyen T M T、Pujolle G等，《C-V2X中的资源分配模式：从LTE-V2X到5G-V2X》，  
IEEE Internet of Things Journal，第9卷第11期，第8291-8314页，2022年。[16]张晨、张玮、吴骞等，《基于分布式深度强化学习的联邦学习车联网梯度量化方法》，IEEE Internet of Things Journal，第1-1页，2024年。[17]吴骞、王伟、范平等，《基于弹性联邦与多智能体深度强化学习的下一代网络协同边缘缓存》，IEEE Transactions  
on Network and Service Management，第21卷第4期，第4179-4196页，2024年。[18]王琴、吴东苏、范平，《无线传感器网络中时延约束下的最优链路调度》，IEEE Transactions on Ve-hicular Technology，第59卷第9期，第4564-4577页，2010年。[19]邓茹、张宇、张浩等，《面向6G超大规模MIMO的可重构全息表面：实用设计、优化与实现》，  
IEEE Journal onSelected Areas in Communications，第41卷第8期，第2367-2379页，2023年。doi:10.1109/JSAC.2023.3288248。[20]林源、赵辉、马骁等，《基于卷积神经网络的调制识别对抗攻击研究》，IEEE  
Transactions on Reliability, 第70卷第1期, 第389-401页, 2021年。doi: 10.1109/TR.2020.3032744。[21] Ashraf S A、Blasco R、Do H等，《5G新空口Release-16系统对车联网服务的支持》，  
IEEE Communications Standards Magazine，第4卷第1期，第26-32页，2020年。[22]张睿、熊凯、杜化等，“生成式AI赋能的车联网：基础、框架与案例研究”，收录于IEEE Network，第38卷第4期，第259-267页，2024年7月。doi:10.1109/MNET.2024.3391767。[23]吴凡、吕飞、吴泓等，“基于用户关联模式特征的小基站边缘系统性能优化”，收录于  
IEEE Network, 第37卷第3期, 第210-217页, 2023年5/6月。doi: 10.1109/MNET.121.2200089。[24] 庄伟、叶清、吕飞等, “SDN/NFV赋能的未来车联网：增强通信、计算与缓存能力”, 收录于Proceedings of the IEEE  
，第108卷第2期，第274-291页，2020年2月。doi: 10.1109/JPROC.2019.2951169。[25]陈寅、常铮、闵刚等，《移动边缘计算中状态更新的感知与计算联合优化》

计算系统，”发表于《  
IEEE Transactions on Wireless Communications》，第22卷第11期，第8230-8243页，2023年11月。doi: 10.1109/TWC.2023.3261338。[26] N. Cheng、F. Lyu、W. Quan、C. Zhou、H. He、W. Shi与X. Shen，“面向物联网应用的星/空辅助计算卸载：一种基于学习的方法”，《  
IEEE Journal on Selected Areas in Com-munications》，第37卷第5期，第1117-1129页，2019年。[27] R. Zhang、K. Xiong、Y. Lu、B. Gao、P. Fan与K. B. Letaief，“多用户MISO SWIPT使能异构网络中的联合协调波束成形与功率分配优化：基于多智能体DDQN的方法”，《  
IEEE Journal on Selected Areas in Communications》，第40卷第2期，第677-693页，2022年。[28] D. Zhai、R. Zhang、L. Cai、B. Li与Y. Jiang，“基于NOMA的大规模物联网设备无线网络能效用户调度与功率分配”  
，《IEEE Internet of Things Journal》，第5卷第3期，第1857-1868页，2018年。[29]R.Zhang、K.Xiong、Y.Lu、P.Fan、D.W.K.Ng与K.B.Let aief，“RIS辅助SWIPT网络中RSMA的能效最大化：一种基于PPO的方法”，收录于《IEEE Journal on Selected Areas in Communications》，第41卷第5期，第1413-1430页，2023年5月。doi:10.1109/JSAC.2023.3240707。[30]S.Yue、S.Zeng、L.Liu、Y.C.Eldar与B.Di，“全息MIMO通信的混合远近场信道估计”，《  
IEEE Transactions on Wireless Communications》，2024年，doi:10.1109/TWC.2024.3433491。[31]Y.Xiao、J.Liu、J.Wu与N.Ansari，“利用深度强化学习进行流量工程：综述”，《IEEE Communications Sur-  
veys & Tutorials》，第23卷第4期，第2064-2097页，2021年。[32] Z. Zhao、S. Bu、T. Zhao、Z. Yin、M. Peng、Z. Ding与T. Q. S. Quek，“雾无线接入网络中计算卸载的设计”，《  
IEEE Transactions on Vehicular Technology》，第68卷第7期，第7136-7149页，2019年。[33]Y.Lin、M.Wang、X.Zhou、G.Ding与S.Mao，“具有优先级的无人机编队通信动态频谱交互：一种深度强化学习方法”  
，收录于《IEEE Transactions on Cognitive Communications and Networking》，第6卷第3期，第892-903页，2020年9月。doi:10.1109/TCCN.2020.2973376。[34]Y.Lin、Y.Tu、Z.Dou、L.Chen与S.Mao，“轮廓星图与深度学习在物理层信号识别中的应用”，收录于《IEEETransactions on Cognitive Communications and Networking》  
，第7卷第1期，第34-46页，2021年3月。doi:10.1109/TCCN.2020.3024610。[35] R. Sun、N. Cheng、C. Li、F. Chen与W. Chen，“6G无线网络优化中知识驱动的深度学习范式”，《IEEE Network》，第38卷第2期，第70-78页，2024年。[36] K. Qi、Q. Wu、P. Fan、N. Cheng、Q. Fan与J. Wang，“基于多智能体强化学习的可重构智能表面辅助车边缘计算”，《IEEE Communications Letters》，2024年，doi:10.1109/LCOMM.2024.3451182。[37] J. Shen、N. Cheng、X. Wang、F. Lyu、W. Xu、Z. Liu、K. Aldubaikhy与X. Shen，“RingSFL：一种应对客户端异构性的自适应分裂联邦学习”，收录于《IEEE Transactions on Mobile Computing》，第23卷第5期，第5462-5478页，2024年5月。doi:10.1109/TMC.2023.3309633。[38] C. Guo、L. Liang与G. Y. Li，“低延迟车联网通信的资源分配：有效容量视角”，《IEEEJournal on Selected Areas in Communications》，第37卷第4期，第905-917页，2019年。[39] M. Zhang与Y. Chen，“基于图神经网络的链路预测”，收录于《  
Neural Information Processing Systems》，2018年。[在线]。可用：https://api.sementricscholar.org/CorpusID:3573161 [40] T. Kipt与M. Welling，“使用图卷积网络的半监督分类”，《ArXiv》，第abs/1609.02907卷，2016年。[在线]。可用：https://api.sementricscholar.org/CorpusID:3144218 [41] M. Zhang、Z. Cui、M. Neumann与Y. Chen，“面向图分类的端到端深度学习架构”，收录于《AAAI Conference on Artificial Intelligence》，2018年。[在线]。可用：https://api.sementricscholar.org/CorpusID:4770492 [42] W. L. Hamilton、Z. Ying与J. Leskovec，“大规模图上的归纳表示学习”，收录于《Neural Information Processing Systems》，2017年。[在线]。可用：https://api.sementricscholar.org/CorpusID:4755450 [43] W. Sun、E. G. Ström、F. Brännström、K. C. Sou与Y. Sui，“基于D2D的车对车通信无线资源管理”，《IEEE Transactions on Vehicular Technology》，第65卷第8期，第6636-6650页，2016年。[44] Q. Wei、L. Wang、Z. Feng与Z. Ding，“LTE-U驱动的异构车联网通信网络无线资源管理”，《IEEE Transactions on Vehicular Technology》，第67卷第8期，第7508-7522页，2018年。

[45] 王鹏、狄博、张辉、边凯与宋令，“非授权频谱中的蜂窝车联网通信：与5G系统中VANET的和谐共存”，  
IEEE Transactions on Wireless Communications，第17卷第8期，第5212-5224页，2018年。[46] 李旭、马龙、徐毅与R. Shankaran，“基于D2D的车联网通信在非完美CSI下的资源分配”，IEEE Internet of Things  
Journal，第7卷第4期，第3545-3558页，2020年。[47]F.Abbas、范平与Z.Khan，“一种基于蜂窝车联网通信的新型低延迟车车资源分配方案”  
，IEEE Transactions on Intelligent Transportation Systems，第20卷第6期，第2185-2197页，2019年。[48]陈伟、戴凌、Khaled B. Letaief与曹政，“协作网络中资源分配的统一跨层框架”，IEEE Trans-  
actions on Wireless Communications，第7卷第8期，第3000-3012页，2008年。[49]张宇捷与Khaled B. Letaief，“多用户分组OFDM网络的自适应资源分配与调度”，发表于2004 IEEE Inter-  
national Conference on Communications (IEEE Cat. No.04CH37577)，第5卷，2004年，第2949-2953页。[50]熊凯、陈超、屈国、范平与Khaled B.Letaief，“无线供能通信网络中具有最优资源分配的群组协作”，  
IEEE Transactions on Wireless Communications，第16卷第6期，第3840-3853页，2017年。[51]李涛、范平、陈志与Khaled B. Letaief，“由移动控制中心供能的能量收集传感器网络最优传输策略”，  
IEEE Transactions on Wireless Communications，第15卷第9期，第6132-6145页，2016年。[52]张军、范平与Khaled B. Letaief，“无线自组网中高效组播路由的网络编码”，IEEE Transactions on Communications，第56卷第4期，第598-607页，2008年。[53]姚珍、江杰、范平、曹政与李硕彦，“基于邻居表的自组网多路径路由”，发表于  
Proc. 57th IEEE Semiannual  
Vehicular Technology Conference, 2003. VTC 2003-Spring, 第3卷, 2003年, 第1739-1743页。[54] 叶辉、李建业与B.-H. F. Juang, “基于深度强化学习的车车通信资源分配”, IEEE Transactions on Vehicular Technology, 第68卷第4期, 第3163-3173页, 2019年。[55] 王旭、张宇、沈瑞、徐毅与郑福春, “基于DRL的上行NOMA系统能效资源分配框架”, IEEE Internet of Things Journal, 第7卷第8期, 第7279-7294页, 2020年。[56] 张旭、彭木根、闫实与孙咏, “基于深度强化学习的蜂窝车联网通信模式选择与资源分配”, IEEE Internet of Things Journal, 第7卷第7期, 第6380-6391页, 2020年。[57] 袁煜、郑国、黄凯与Khaled B. Letaief, “基于元强化学习的动态车联网通信资源分配”,  
IEEE Transactions on Vehicular Technology, 第70卷, 第8964-8977页, 2021年。[在线]。可用: https://apisemanticscholar.org/ CorpusID:237600245 [58] 李敏、余国与李建业, “基于图嵌入的少量训练样本无线链路调度”, IEEE Transactions on Wireless Communications, 第20卷第4期, 第2282-2294页, 2021年。[59] 陈涛、张旭、游明、郑国与S. Lambotharan, “基于GNN的无线物联网网络资源分配监督学习框架”,  
IEEE Internet of Things Journal，第9卷第3期，第1712-1724页，2022年。[60]郭静与杨辰，“基于异质图神经网络的多小区多用户系统功率分配学习”，IEEE Transactions on Wireless Communications，第21卷第2期，第884-897页，2022年。[61]邵振、吴强、范平、程宁、陈伟、王军与Khaled B. Letaief，“基于深度强化学习的车联网语义感知频谱共享”，IEEE Internet of Things Journal，第1-1页，2024年。[62]“第三代合作伙伴计划；无线接入网技术规范组；演进的通用陆地无线接入；E-UTRA物理层方面的进一步增强（第9版）”。[在线]。可用：https://api-semanticscholar.org/CorpusID:16652630 [63]M.I.Ashraf、M.Bennis、C.Perfecto与W.Saad，“车车通信中基于动态邻近感知的资源分配”，发表于  
2016 IEEE Globecom Workshops (GC Wkshps)，2016年，第1-6页。[64]郑政、施晓、何龙、金海、魏松、戴皓与彭晓，“Feluca：GPU上以颜色为中心的两阶段图着色算法”，发表于  
IEEE Transactions on Parallel and Distributed Systems，第32卷第1期，第160-173页，2021年1月1日。doi：10.1109/TPDS.2020.3014173。[65]郑政、杜宾、赵超与谢鹏，“延迟容忍网络中基于路径合并的介数中心性算法”，发表于IEEE Journal on Selected Areas in Communications，第41卷第10期，第3133-3145页，2023年10月。doi：10.1109/JSAC.2023.3310071